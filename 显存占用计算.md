### 内存总量 
```python
device = torch.cuda.get_device_properties(i) # 获取索引为 i 的 CUDA 设备的属性信息
total_memory_gb = round(device.total_memory / (1024**3), 1) # device.total_memory  获取设备的总内存大小
print(f"Total GPU memory: {total_memory_gb} GB")
```
### 参数占用显存
#### 模型的总参数
```python
total_parameters = model.num_parameters()
```
#### 模型占用显存
以float16为例， 16个bit，2个byte
```python
total_memory_bytes = total_parameters *  2 byte #（以字节为单位）根据总参数数量计算模型所需的显存大小
total_memory_gb = total_memory_bytes / (1024**3) # （GB）
```

### 查看显存占用
```python
memory_allocated = torch.cuda.memory_allocated(device='cuda:0')  # 获取指定 CUDA 设备上已分配的显存大小，以字节为单位。
memory_allocated_gb = memory_allocated / (1024**3) # （GB）
```

### 模型精度转换
#### 手动
```
model.half() / input_data.half()
float()
bfloat16()
```

如 float16-> float32：
```python
memory_allocated_before = torch.cuda.memory_allocated(device='cuda:0')
model.float()
memory_allocated_after = torch.cuda.memory_allocated(device='cuda:0')
```

#### AMP (Automatic Mixed Precision)
AMP 是一种自动混合精度工具，可以自动处理 float32 和 float16 的转换，以实现显存节省和计算加速
在推理阶段:
```
from torch.cuda.amp import autocast
# 使用 AMP 进行推理
with torch.no_grad():
    with autocast():
        output = model(input_data)
```
### 浮点数
#### IEEE 754 标准

| 格式       | 总位数 | 符号位 | 指数位 | 尾数位|偏置值 | 数值范围（指数部分） | 数据范围          | 
|------------|--------|--------|--------|--------|--------|-----------------------|-------------------|
| float32    | 32     | 1      | 8      | 23     |127     |  -126 到 127           | ±1.18e-38 到 ±3.4e38 |
| float16    | 16     | 1      | 5      | 10     | 15     | -14 到 15             | ±5.96e-8 到 ±65504  |   
| bfloat16   | 16     | 1      | 8      | 7      | -126 到 127           | ±1.18e-38 到 ±3.4e38 |     
| int8       | 8      | 1     | 无     | 无     | 无                    | -128 到 127（有符号） |   
|            |        |        |        |        |                       | 0 到 255（无符号）   |      

- 符号位[Sign]：符号位决定数值的正负。当符号位为 0 时，表示正数；当符号位为 1 时，表示负数。
- 指数位[exponent]：指数位用于表示数值的大小范围。通常采用偏置表示法，其中指数部分的实际值为指数位的二进制表示减去偏置值。偏置值通常是 $2^{(n-1)} - 1$，其中 $n$ 是指数位数。
- 尾数位[Fraction]：尾数位用于存储有效数值的精度部分。在浮点数表示中，尾数部分通常隐含一个二进制的 "1"，即隐含位，因此尾数部分的实际精度会比尾数位数多一个有效位。

公式：
$(-1)^{\text{符号位}} \times 1.\text{尾数位} \times 2^{(\text{指数位} - \text{偏置})}$，其中偏置是指数位的偏置值。

#### torch.finfo 
获取给定浮点数类型的机器精度信息


> - resolution：表示分辨率，通常与 eps 相同。它表示在 1.0 和 1.0 加上最小可表示增量之间的差值。
>- min：最小负值。
>- max：最大正值。
>- eps：机器精度，表示浮点数类型能够表示的最小增量。反映了浮点数表示的最小步长
>- smallest_normal：最小的标准化浮点数（大于非标准化浮点数）。
>- tiny：最小正数（非标准化浮点数）

例子：
```
import torch

# 查看 float32 的信息
finfo_float32 = torch.finfo(torch.float32)
print(f"float32:")
print(f"  精度: {finfo_float32.eps}")
print(f"  最小正数: {finfo_float32.tiny}")
print(f"  最小值: {finfo_float32.min}")
print(f"  最大值: {finfo_float32.max}")

```

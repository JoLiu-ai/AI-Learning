### thought tokens

### test time compute
在模型推理阶段，尤其是在使用大型语言模型（LLMs）时，分配和利用计算资源以提高输出质量的过程
两种主要的优化机制：
- 基于验证者奖励模型的搜索：这种方法涉及为给定提示生成多个候选响应，然后使用训练好的奖励模型对这些响应进行评估，从中选择最佳选项。这种方法在处理复杂提示时特别有效，因为多种视角可以提升最终输出的质量。
- 自适应提议分布修改：在这种策略中，模型会对初始响应进行迭代性地精炼，根据之前输出的反馈进行调整。这种动态调整使得模型能够实时改善其预测，对于较简单的问题尤其有利，因为从先前尝试中学习可以显著提高准确性。

## 核心思想：
用容易采样的分布（q(x)）代替难以直接采样的目标分布（p(x)），通过加权样本修正偏差。

## 目标期望的转换

目标分布 $p(x)$ 下函数 $f(x)$ 的期望：  


$$ 
E_p[f(x)] = \int f(x)p(x)dx 
$$

引入建议分布 $q(x)$，改写为：

$$ 
E_p[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x)dx = E_q \left[ f(x) \frac{p(x)}{q(x)} \right] 
$$

**重要性权重**：  

$$ 
w(x) = \frac{p(x)}{q(x)} 
$$

用于调整从 $q(x)$ 采样的偏差。

## 蒙特卡洛估计

从 $q(x)$ 采样 $N$ 个样本 $x_i$，估计期望：

$$ 
E_p[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i) \cdot w(x_i) 
$$

## 关键注意事项

### 1. 分布差异控制
- **方差问题**：当目标分布 $p(x)$ 与建议分布 $q(x)$ 差异过大时，重要性权重 $w(x)$ 的方差会急剧增加，导致估计不准确。
- **解决方法**：
  - **KL散度约束**（如TRPO算法）：
    限制新旧策略之间的KL散度 $D_{KL}(p \parallel q) \leq \delta$，确保分布相似性。
  - **比率截断**（如PPO算法）：
    将重要性比率 $r(\theta) = \frac{p(x)}{q(x)}$ 限制在 $[1 - \epsilon, 1 + \epsilon]$ 范围内，避免过大的策略更新。

### 2. 权重归一化
- **问题**：直接使用原始权重 $w(x_i)$ 可能导致数值不稳定（如样本量小时权重极端化）。
- **改进方法**：
  使用归一化权重：
  $\hat{w}(x_i) = \frac{w(x_i)}{\sum_{j=1}^N w(x_j)}$
  通过归一化使权重总和为1，提升估计稳定性，尤其适用于小样本场景。

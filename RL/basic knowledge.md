# 1. åŸºç¡€æ¦‚å¿µ
## 1.1 State, s
çŠ¶æ€ï¼Œç³»ç»Ÿåœ¨æŸä¸€æ—¶åˆ»çš„è§‚æµ‹å€¼æˆ–ç¯å¢ƒä¿¡æ¯ï¼Œå¯ä»¥æ˜¯ç¦»æ•£çš„ï¼ˆå¦‚æ£‹ç›˜ä½ç½®ï¼‰æˆ–è¿ç»­çš„ï¼ˆå¦‚æœºå™¨äººåæ ‡ï¼‰
## 1.2 Action, a
åŠ¨ä½œ ï¼Œæ™ºèƒ½ä½“åœ¨çŠ¶æ€ sä¸‹æ‰§è¡Œçš„æ“ä½œï¼Œå¯ä»¥æ˜¯ç¦»æ•£ï¼ˆå¦‚å·¦/å³ç§»åŠ¨ï¼‰æˆ–è¿ç»­ï¼ˆå¦‚æ–½åŠ åŠ›çš„å¤§å°ï¼‰
state transition
![image](https://github.com/user-attachments/assets/b51576ba-748f-4647-9432-c7f335fe9ce0)
## 1.3 Policy, Ï€
æ ¹æ®è§‚æµ‹åˆ°çš„çŠ¶æ€ï¼Œå¦‚ä½•åšå‡ºå†³ç­–ï¼Œå³å¦‚ä½•ä»åŠ¨ä½œç©ºé—´ä¸­é€‰å–ä¸€ä¸ªåŠ¨ä½œã€‚
the probability of taking action a given state s
Ï€(a|s) =P(A=a|S=s)
- ç¡®å®šæ€§ç­–ç•¥: ç¡®å®šç­–ç•¥è®°ä½œ Âµ : S -> Aï¼Œå®ƒæŠŠçŠ¶æ€ s ä½œä¸ºè¾“å…¥ï¼Œç›´æ¥è¾“å‡ºåŠ¨ä½œ a = Âµ(s)ï¼Œ
è€Œä¸æ˜¯è¾“å‡ºæ¦‚ç‡å€¼ã€‚
![image](https://github.com/user-attachments/assets/5f9b0ed7-44ee-4c45-9883-e399b7aff66d)
- éšæœºæ€§ç­–ç•¥: ä»¥æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©åŠ¨ä½œ
  - ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼š
  - è¿ç»­åŠ¨ä½œç©ºé—´ï¼š æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆProbability Density Function, PDFï¼‰
![image](https://github.com/user-attachments/assets/68ee8be4-39ac-4bb8-89f4-baf6a493473e)

## 1.4 Reward R
å¥–åŠ±ä¸å›æŠ¥


## 1.5 Value Function
- çŠ¶æ€å€¼å‡½æ•°V(s): çŠ¶æ€sçš„æœŸæœ›é•¿æœŸæ”¶ç›Š
- åŠ¨ä½œå€¼å‡½æ•°Q(s,a): åœ¨çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaçš„æœŸæœ›æ”¶ç›Šï¼š

### 1.5.1 discounted return:  aka cumulative discounted future reward
ä» t æ—¶åˆ»èµ·ï¼Œæœªæ¥æ‰€æœ‰å¥–åŠ±çš„ï¼ˆåŠ æƒï¼‰å’Œ
![image](https://github.com/user-attachments/assets/9ba19c77-6f9e-4443-9bc5-ee5bc98a33dd)

### 1.5.2 action-value function for policy ğ¿
å¯¹ $$U_t$$ æ±‚æœŸæœ›ï¼Œæ¶ˆé™¤æ‰å…¶ä¸­çš„éšæœºæ€§
![image](https://github.com/user-attachments/assets/d0307110-ea5d-4596-bee9-5bc8f080b200)

### 1.5.3 optimal action-value function
é€‰æ‹©æœ€å¥½çš„ç­–ç•¥å‡½æ•° Ï€
![image](https://github.com/user-attachments/assets/177dcd3c-d930-4dd1-9abd-c647c70d722c)
![image](https://github.com/user-attachments/assets/6e3581c3-3086-47ca-8e18-283f983be7a8)

### 1.5.4 state-value function
ç›®æ ‡ï¼šè¯„ä¼°å½“å‰çŠ¶æ€ $$s_t$$ æ˜¯å¦å¯¹è‡ªå·±æœ‰åˆ©
æŠŠåŠ¨ä½œ $$A_t$$ ä½œä¸ºéšæœºå˜é‡ï¼Œç„¶åå…³äº $$A_t$$ æ±‚æœŸæœ›ï¼ŒæŠŠ  $$A_t$$ æ¶ˆæ‰
![image](https://github.com/user-attachments/assets/261fa598-d004-4c07-92c3-111bfdf88456)

### 1.6 advantage
 $$A(s,a)=Q(s,a)âˆ’V(s)$$ 
é€‰å–æŸä¸ªåŠ¨ä½œåå¾—åˆ°çš„åŠ¨ä½œä»·å€¼ç›¸å¯¹äºå¹³å‡åŠ¨ä½œä»·å€¼çš„å¢é‡ï¼Œå½“ä¼˜åŠ¿å‡½æ•°å¤§äº0æ—¶ï¼Œè¯´æ˜å½“å‰çš„åŠ¨ä½œé€‰æ‹©æ˜¯ä¼˜äºå¹³å‡çš„

# å…¶ä»–æ¦‚å¿µã€todoã€‘
é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov decision processï¼Œ MDPï¼‰

# å‚è€ƒ
1. æ·±åº¦å¼ºåŒ–å­¦ä¹ /Deep Reinforcement Learning ç‹æ ‘æ£® é»å½§å› å¼ å¿—å è‘—

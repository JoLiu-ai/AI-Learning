```python
import numpy as np
import hashlib
import json
from typing import List, Dict, Any, Union
from sentence_transformers import SentenceTransformer
import torch

class StableVectorizer:
    """解决向量化玄学问题的稳定向量化器"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.model.eval()  # 确保模型在评估模式
        
        # 固定随机种子
        torch.manual_seed(42)
        np.random.seed(42)
        
        # 缓存机制
        self.vector_cache = {}
        self.preprocessing_config = {
            "lowercase": True,
            "strip_whitespace": True,
            "normalize_unicode": True,
            "remove_extra_spaces": True
        }
    
    def _preprocess_text(self, text: str) -> str:
        """标准化文本预处理"""
        if not isinstance(text, str):
            text = str(text)
        
        # 1. Unicode标准化
        import unicodedata
        text = unicodedata.normalize('NFKC', text)
        
        # 2. 去除首尾空白
        text = text.strip()
        
        # 3. 统一大小写（可选）
        if self.preprocessing_config["lowercase"]:
            text = text.lower()
        
        # 4. 处理多余空格
        if self.preprocessing_config["remove_extra_spaces"]:
            import re
            text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def _get_text_hash(self, text: str) -> str:
        """生成文本的哈希值用于缓存"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def encode_single(self, text: str, use_cache: bool = True) -> np.ndarray:
        """单个文本向量化"""
        # 预处理
        processed_text = self._preprocess_text(text)
        
        # 检查缓存
        if use_cache:
            text_hash = self._get_text_hash(processed_text)
            if text_hash in self.vector_cache:
                return self.vector_cache[text_hash].copy()
        
        # 向量化
        with torch.no_grad():
            vector = self.model.encode(
                processed_text,
                convert_to_tensor=False,
                normalize_embeddings=True,  # 标准化向量
                batch_size=1  # 确保批处理一致性
            )
        
        # 确保数据类型一致
        vector = np.array(vector, dtype=np.float32)
        
        # 缓存结果
        if use_cache:
            self.vector_cache[text_hash] = vector.copy()
        
        return vector
    
    def encode_batch(self, texts: List[str], use_cache: bool = True) -> np.ndarray:
        """批量文本向量化"""
        # 预处理所有文本
        processed_texts = [self._preprocess_text(text) for text in texts]
        
        # 检查缓存
        cached_vectors = []
        uncached_texts = []
        uncached_indices = []
        
        if use_cache:
            for i, text in enumerate(processed_texts):
                text_hash = self._get_text_hash(text)
                if text_hash in self.vector_cache:
                    cached_vectors.append((i, self.vector_cache[text_hash].copy()))
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)
        else:
            uncached_texts = processed_texts
            uncached_indices = list(range(len(texts)))
        
        # 批量处理未缓存的文本
        all_vectors = [None] * len(texts)
        
        if uncached_texts:
            with torch.no_grad():
                new_vectors = self.model.encode(
                    uncached_texts,
                    convert_to_tensor=False,
                    normalize_embeddings=True,
                    batch_size=32,  # 固定批次大小
                    show_progress_bar=False
                )
            
            # 确保数据类型一致
            new_vectors = np.array(new_vectors, dtype=np.float32)
            
            # 存储新向量
            for i, (orig_idx, vector) in enumerate(zip(uncached_indices, new_vectors)):
                all_vectors[orig_idx] = vector
                
                # 缓存结果
                if use_cache:
                    text_hash = self._get_text_hash(processed_texts[orig_idx])
                    self.vector_cache[text_hash] = vector.copy()
        
        # 填入缓存的向量
        for orig_idx, vector in cached_vectors:
            all_vectors[orig_idx] = vector
        
        return np.array(all_vectors)
    
    def verify_consistency(self, text: str, n_trials: int = 5) -> Dict[str, Any]:
        """验证向量化一致性"""
        results = {
            "text": text,
            "vectors": [],
            "similarities": [],
            "is_consistent": True,
            "max_difference": 0.0
        }
        
        # 多次向量化同一文本
        for _ in range(n_trials):
            vector = self.encode_single(text, use_cache=False)
            results["vectors"].append(vector)
        
        # 计算相似度
        base_vector = results["vectors"][0]
        for i in range(1, len(results["vectors"])):
            similarity = np.dot(base_vector, results["vectors"][i]) / (
                np.linalg.norm(base_vector) * np.linalg.norm(results["vectors"][i])
            )
            results["similarities"].append(similarity)
            
            # 计算最大差异
            diff = np.max(np.abs(base_vector - results["vectors"][i]))
            results["max_difference"] = max(results["max_difference"], diff)
        
        # 判断是否一致（允许浮点误差）
        threshold = 1e-6
        results["is_consistent"] = results["max_difference"] < threshold
        
        return results
    
    def batch_vs_single_test(self, texts: List[str]) -> Dict[str, Any]:
        """测试批处理vs单独处理的一致性"""
        results = {
            "texts": texts,
            "single_vectors": [],
            "batch_vectors": None,
            "similarities": [],
            "is_consistent": True
        }
        
        # 单独处理
        for text in texts:
            vector = self.encode_single(text, use_cache=False)
            results["single_vectors"].append(vector)
        
        # 批处理
        results["batch_vectors"] = self.encode_batch(texts, use_cache=False)
        
        # 比较一致性
        for i in range(len(texts)):
            single_vec = results["single_vectors"][i]
            batch_vec = results["batch_vectors"][i]
            
            similarity = np.dot(single_vec, batch_vec) / (
                np.linalg.norm(single_vec) * np.linalg.norm(batch_vec)
            )
            results["similarities"].append(similarity)
            
            # 检查是否一致
            if similarity < 0.9999:  # 允许极小的数值误差
                results["is_consistent"] = False
        
        return results

# 使用示例
if __name__ == "__main__":
    # 创建稳定向量化器
    vectorizer = StableVectorizer()
    
    # 测试一致性
    test_text = "苹果公司发布了新款iPhone"
    consistency_result = vectorizer.verify_consistency(test_text)
    print(f"一致性测试结果: {consistency_result['is_consistent']}")
    print(f"最大差异: {consistency_result['max_difference']}")
    
    # 测试批处理vs单独处理
    test_texts = [
        "苹果公司发布了新款iPhone",
        "谷歌推出了新的AI模型",
        "微软发布了Windows更新"
    ]
    
    batch_test = vectorizer.batch_vs_single_test(test_texts)
    print(f"批处理一致性: {batch_test['is_consistent']}")
    print(f"相似度: {batch_test['similarities']}")
    
    # 演示上下文敏感性问题
    print("\n=== 上下文敏感性测试 ===")
    context_tests = [
        "苹果",  # 无上下文
        "苹果公司股价上涨",  # 科技上下文
        "红苹果很甜",  # 水果上下文
        "苹果树结果了",  # 植物上下文
    ]
    
    vectors = vectorizer.encode_batch(context_tests)
    
    # 计算"苹果"在不同上下文中的相似度
    base_vector = vectors[0]  # 无上下文的"苹果"
    for i, context in enumerate(context_tests[1:], 1):
        similarity = np.dot(base_vector, vectors[i]) / (
            np.linalg.norm(base_vector) * np.linalg.norm(vectors[i])
        )
        print(f"'{context}' 与基准相似度: {similarity:.4f}")
```

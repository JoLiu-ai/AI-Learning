- 数据并行 vs. 模型并行
    - 数据并行：模型拷贝（per device），数据batch 上拆分，如batch_size=32，每个device16
    - 模型并行：数据拷贝（per device），模型拆分
 
### 类型
-  数据并行 ：数据并行DP – 分布式数据并行 DDP – 分片共享数据并行 FSDP
-  模型并行：模型并行MP - 张量并行 TP – 流水线并行 PP
-  自动并行： MindSpore 张量自动并行
- 多维混合并行：Embedding并行 – 数据并行&模型并行 MPDP

### Data parallelism

- Data parallelism, DP  ```nn.DataParallel```
- Distribution Data Parallel, DDP ```DistributedDataParallel```
- Fully Sharded Data Parallel, FSDP

#### DP




```
`nn.DataParallel
device_ids=None, 默认是所有gpus
output_device=None，默认gpus[0]
dim=0
The parallelized module must have its parameters and buffers on device_ids[0] before running(forward/backward) this DataParallel module.
model.to('cuda:0')
```

### 代码
```
device = torch.device("cuda: 1,2"） ## specify the GPu id' s
model = CreateModel()
model= nn.DataParallel(model, device_ids=[1, 2])
model.to(device)
```

### DDP
Ring AllReduce algorithm
- 百度提出来的；
- 环形的，logical 的（逻辑的，非物理的）
- 两个过程（基于环形逻辑拓扑）
   -  scatter-reduce
   -  all gather（broadcast）
     

# 资料
1. [ZOMI酱](https://space.bilibili.com/517221395)


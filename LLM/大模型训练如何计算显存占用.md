# 基础

- **K** 千 (kilo) = 10^3
- **M** 兆(mega) = 10^6
- **G** 十亿 (giga)= 10^9
- **T 太（拉），兆兆，万亿**（tera）= 10^12

1GB = 1024MB

1MB = 1024KB

1KB = 1024Byte

1Byte字节 = 8Bit 比特

FP32： 32Bits, 4Bytes

FP16： 16Bits, 2Bytes

# 参数

## 1. 输入输出

Lama13B

设定：

- b Batch Size: 1
- s Sequence Len : 1024
- h Hidden Size: 5120

Embedding后输入：`b * s * h * 2/1024/1024 （2：2Bytes，FP16）`

FP16: 10MB

输入输出：10MB【输入】 + 10MB【输出-与输入一致】 = 20MB

## 2. 模型参数

Llama13B

***B*** = one billion（10亿）

1***B*** = 1000^3 ≈  1024^3byte（1GB）

FP16: `13*2 * 1B=26GB`

## 3. 优化器

Llama13B      

以Adam 为例，**float32 存储**：

- 梯度指数平滑值：13 * 4= 52GB
- 梯度平方指数平滑值：13 * 4=52GB
- 优化器存储：模型参数  13*4= 52GB
    
    一共156GB
    

🙋 问题：

- 为什么优化器这里都是FP32？
- 为什么优化器还要存一份模型参数？

📖 答案：小值累加操作，FP16会丢失精度，太小的值会被表示为0

pytorch sum / mean 会自动转为FP32进行操作

## 4. 激活值

![image](https://github.com/hinswhale/AI-Learning/assets/22999866/7ea8d498-df72-4164-b73b-36cd1ad73600)

反向传播需要**`前向计算中的中间结果`**，为了计算梯度方便，**中间结果都会放在显存中**

Llama13B

s 序列长度：1024

b batch size: 1

h 隐藏层大小：5120

a Attention: 40

L层数：40

FP16: `s * b * h （34 + 5 * a* * s/h）* L/1024/1024/1024 GB =14.5GB`


⚠️：公式中 *s * s 序列长度增加，激活值占用空间以平方增长*

## 5. 梯度值

Llama13B

FP16: 13B*2=26G

## 汇总

Llama13B FP16

> - b Batch Size: 1
>- s Sequence Len : 1024
>- h Hidden Size: 5120
>- a Attention: 40
>- L层数：40

输入输出：b* s * h * 2/1024/1024 

模型参数：26GB

优化器：156GB

激活值：14.5G （和BatchSize, Seq_Len相关）

梯度值：26GB

合计：222.5GB

  
|  | 公式 | 影响因素 | 数据类型 |
| --- | --- | --- | --- |
| 输入输出 | **b * s * h**  | b / s / h / 数据类型 | FP16 |
| 模型参数 | 模型参数个数 *数据类型   | 模型参数个数/ 数据类型 | FP16 |
| 优化器 | 梯度指数平滑值：<br>梯度平方指数平滑值<br>优化器存储  | 模型参数个数/ 数据类型 | FP32 |
| 激活值 | s / b / h / Attention头 / s / L层数| 和BatchSize, Seq_Len相关 | FP16 |
| 梯度值 | 模型参数个数 *数据类型  | 模型参数个数/ 数据类型 |  |


# 参考文献
1.[大模型训练如何计算显存占用-RethinkFun](https://www.bilibili.com/video/BV1VD421571H)

ğŸ¯ç›®æ ‡ï¼šè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»åå¥½ï¼Œå°†äººç±»çš„ä¸»è§‚åå¥½åˆ¤æ–­è½¬åŒ–ä¸ºå¯é‡åŒ–çš„å¥–åŠ±ä¿¡å·  
ğŸ”§æ–¹æ³•ï¼šæ”¶é›†äººç±»å¯¹è¾“å‡ºè´¨é‡çš„åå¥½åˆ¤æ–­ï¼Œå¦‚"Aæ¯”Bå¥½"è¿™æ ·çš„æˆå¯¹æ¯”è¾ƒæ•°æ®  
âœ…æ¶‰åŠåˆ°çš„3ä¸ªmodelï¼šLanguage Model(LM), Tuned Language Model (RL Policy),Reward (Preference) Model   
  Language Model(LM) - ç”Ÿæˆå†…å®¹ï¼›  
  Reward Modelï¼ˆRMï¼‰- è¿›è¡Œè¯„ä¼°å¹¶æä¾›åé¦ˆï¼›  
  Tuned Language Model (RL Policy) - ä»äººç±»çš„åå¥½ä¸­å­¦ä¹ ï¼Œè¾…åŠ©å¥–åŠ±æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæå‡è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚  

âœ… ä¼ ç»ŸRLHFæ–¹æ³•: Involves three model (reward model, reference model, and the fine-tuned model)
âœ… DPO: Only involves the fine-tuned model.(Direct Preference Optimization: Your Language Model is Secretly a Reward Model)

![image](https://github.com/user-attachments/assets/81ba0cd8-b043-4868-b43b-1561c2d4c07b)


âœ… two loss items


### ğŸ“Œä¼ ç»ŸRLHFæ–¹æ³•ï¼š
<img width="571" alt="æˆªå±2024-12-04 18 03 41" src="https://github.com/user-attachments/assets/92a035cf-19fb-4fc0-8d1c-c1272befed63">

- æµç¨‹ï¼š 
  - é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ Ï€_ref 
  - æ”¶é›†åå¥½æ•°æ® D = {(x, y_w, y_l)} 
  - è®­ç»ƒå¥–åŠ±æ¨¡å‹ r_Î¸(x,y) 
  - é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ 

- ç›®æ ‡å‡½æ•°ï¼š
  æœ€å¤§åŒ–æœŸæœ›å¥–åŠ± E[R(x,y)]
- æŸå¤±å‡½æ•°ï¼š
  ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼š L(Ï€) = E[(r_Î¸(x,y) - Î² * KL(Ï€(Â·|x) || Ï€_ref(Â·|x))))]


### ğŸ“ŒDPO
DPO converts the problem into a single-phase SFT task.
![image](https://github.com/user-attachments/assets/c2261119-1073-4082-adec-8ab11b3550e9)

ä¸»è¦åŒºåˆ«ï¼š

RLHFéœ€è¦å¤æ‚çš„å¥–åŠ±å»ºæ¨¡å’Œç­–ç•¥æ¢¯åº¦
DPOç›´æ¥é€šè¿‡åå¥½æ•°æ®ä¼˜åŒ–æ¨¡å‹
è®¡ç®—æ›´ç®€å•ã€æ•ˆç‡æ›´é«˜
é¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„ä¸ç¨³å®šæ€§

##
### Prompt Tuning
- From discrete prompt to continuous prompt
The Power of Scale for Parameter-Efficient Prompt Tuning [Lester, ACL 2021]
![image](https://github.com/user-attachments/assets/046dda28-2711-4540-9771-c808a96bd933)


### Prefix-Tuning
- Prepend prefixes for each input.
- Prefix-Tuning: Optimizing Continuous Prompts for Generation [Li et al, ACL 2021]

- Prompt-Tuning only adds learnable prompts to the first layer.
- Prefix-Tuning adds tunable prompts to each layer.

  downsides:
  increase input length & the KV cache

  ## LoRA family - without introudcing extra inference latency

##  Bit-Delta
tune one bit.


### QLoRA
- quantize the base model to 4-bits, like a 7-billion parameter model.
- page the optimizer with the CPU offloading
  

![image](https://github.com/user-attachments/assets/c45c5520-a043-45e7-9625-9595433f365c)
(from: https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part28.html)



1/ https://huggingface.co/blog/rlhf  
2/ [Umar Jamil](https://www.youtube.com/watch?v=hvGa5Mba4c8)

ğŸ¯ç›®æ ‡ï¼šè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»åå¥½ï¼Œå°†äººç±»çš„ä¸»è§‚åå¥½åˆ¤æ–­è½¬åŒ–ä¸ºå¯é‡åŒ–çš„å¥–åŠ±ä¿¡å·  
ğŸ”§æ–¹æ³•ï¼šæ”¶é›†äººç±»å¯¹è¾“å‡ºè´¨é‡çš„åå¥½åˆ¤æ–­ï¼Œå¦‚"Aæ¯”Bå¥½"è¿™æ ·çš„æˆå¯¹æ¯”è¾ƒæ•°æ®  

### RLHFæ•´ä½“æµç¨‹ï¼š
![image](https://github.com/user-attachments/assets/1ff6026a-21db-4a55-8ec8-28947a95ea75)

- ä¼ªä»£ç 
```python
class RLHF_Pipeline:
    def __init__(self):
        self.base_llm = None        # åŸºç¡€è¯­è¨€æ¨¡å‹
        self.sft_model = None       # SFTæ¨¡å‹
        self.reward_model = None    # å¥–åŠ±æ¨¡å‹
        self.rl_model = None        # RLæœ€ç»ˆæ¨¡å‹
        
    def training_flow(self):
        # 1. é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹
        self.pretrain_base_model()

        # 2. SFTé˜¶æ®µ
        self.sft_model = self.supervised_finetuning(self.base_llm)
        
        # 2. Reward Modelé˜¶æ®µ
        self.reward_model = self.train_reward_model()
        
        # 3. RLé˜¶æ®µ
        self.rl_model = self.rl_training(self.sft_model, self.reward_model)
```
#### SFT (Supervised Fine-Tuning)ï¼š
- ç›®çš„: ä½¿æ¨¡å‹åˆæ­¥å¯¹é½äººç±»æ„å›¾
- æ•°æ®: é«˜è´¨é‡äººå·¥ç¼–å†™çš„é—®ç­”å¯¹
- æ–¹æ³•: Supervised
- ç‰¹ç‚¹:ä½œä¸ºRMå’ŒRLçš„åŸºç¡€æ¨¡å‹
```python

class SFT:
    def __init__(self):
        self.model = None
        
    def training_step(self, prompt, response):
        """æ ‡å‡†çš„æœ‰ç›‘ç£å­¦ä¹ """
        output = self.model(prompt)
        loss = self.compute_loss(output, response)
        return loss
```
#### B. Reward Modelè¯¦è§£ï¼š
æµç¨‹ï¼š
- æ•°æ®æ„å»º
  - æ”¶é›†å¤šæ ·åŒ–çš„æç¤º
  - response_generationï¼šä½¿ç”¨SFTæ¨¡å‹ç”Ÿæˆå¤šä¸ªå›ç­”
- è®­ç»ƒç­–ç•¥
  - loss_typeï¼šBradley-Terryæ¨¡å‹/äº¤å‰ç†µ
  - samplingï¼šé‡è¦æ€§é‡‡æ ·
  - regularizationï¼šé¿å…è¿‡æ‹Ÿåˆçš„ç­–ç•¥
- è¯„ä¼°æŒ‡æ ‡
  -  preference_accuracy åå¥½é¢„æµ‹å‡†ç¡®ç‡
  -  ranking_correlationæ’åºç›¸å…³æ€§
  -  human_alignment ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§

```python
# å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒ
class RewardModel:
    def __init__(self):
        self.model = None

    def train_step(self, batch):
        # 1. è·å–ä¸€å¯¹æ¯”è¾ƒæ•°æ®
        prompt, better_resp, worse_resp = batch
        
        # 2. è®¡ç®—å¥–åŠ±åˆ†æ•°
        score_better = self.model(prompt, better_resp)
        score_worse = self.model(prompt, worse_resp)
        
        # 3. è®¡ç®—æŸå¤± - åå¥½å­¦ä¹ 
        loss = -torch.log(torch.sigmoid(score_better - score_worse))
        
        # 4. ä¼˜åŒ–
        loss.backward()
        self.optimizer.step()
    
    def get_reward(self, prompt, response):
        # æ¨ç†æ—¶è®¡ç®—å•ä¸ªå›ç­”çš„å¥–åŠ±
        return self.model(prompt, response)

    def collect_preferences(self):
        # æ”¶é›†äººç±»åå¥½æ•°æ®çš„æ–¹æ³•:
        # 1. å¯¹æ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›ç­”
        responses = self.generate_responses(prompt)
        
        # 2. äººç±»æ ‡æ³¨åå¥½
        # - ç›´æ¥æ¯”è¾ƒä¸¤ä¸ªå›ç­”
        # - å¯¹å¤šä¸ªå›ç­”æ’åº
        # - ç»™å‡ºå…·ä½“åˆ†æ•°
        preferences = self.human_annotate(responses)
        
        return preferences
```

#### PPOTraining
- è½¨è¿¹æ”¶é›†
 - rolloutï¼šä½¿ç”¨å½“å‰ç­–ç•¥é‡‡æ ·
 - reward_computationï¼šä½¿ç”¨RMè®¡ç®—å¥–åŠ±
 - advantage_estimationï¼šGAEè®¡ç®—ä¼˜åŠ¿
- ç­–ç•¥æ›´æ–°
  - policy_loss PPO-Clipç›®æ ‡
  - value_loss ä»·å€¼å‡½æ•°ä¼°è®¡
  - kl_penalty KLæ•£åº¦çº¦æŸ
- å…³é”®æŠ€å·§
  - value_clipping é™åˆ¶ä»·å€¼ä¼°è®¡æ›´æ–°
  - gradient_clipping æ¢¯åº¦è£å‰ª
  - mini_batch å°æ‰¹é‡è®­ç»ƒ


```python

class PPOTraining:
    def __init__(self):
        self.policy = None          # ç­–ç•¥ç½‘ç»œ
        self.value = None           # ä»·å€¼ç½‘ç»œ
        self.reward_model = None    # å¥–åŠ±æ¨¡å‹
        
    def get_reward(self, state, action):
        # ä½¿ç”¨reward modelè®¡ç®—å¥–åŠ±
        return self.reward_model(state, action)
    
    def training_step(self):
        # 1. æ”¶é›†è½¨è¿¹
        trajectories = self.collect_trajectories()
        
        # 2. è®¡ç®—å¥–åŠ±
        rewards = [self.get_reward(s, a) for s, a in trajectories]
        
        # 3. æ›´æ–°ç­–ç•¥
        self.update_policy(trajectories, rewards)
```
#### é«˜çº§æŠ€å·§å’Œä¼˜åŒ–
##### A. æ··åˆè®­ç»ƒç­–ç•¥ï¼š
- SFT+RMæ··åˆ
  - åŒæ—¶è®­ç»ƒSFTå’ŒRM,æé«˜æ¨¡å‹æ•ˆç‡å’Œæ€§èƒ½
- æ¸è¿›å¼è®­ç»ƒ
  - é€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦,æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
- å¤šä»»åŠ¡è®­ç»ƒ
  - åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡,æ›´å…¨é¢çš„èƒ½åŠ›æå‡

##### B. ä¼˜åŒ–ç­–ç•¥ï¼š
- å¥–åŠ±å¡‘å½¢
 - è®¾è®¡å¤åˆå¥–åŠ±å‡½æ•°, åŸºç¡€å¥–åŠ± KLæƒ©ç½š å¤šæ ·æ€§å¥–åŠ±
- æ ·æœ¬æ•ˆç‡
 - æé«˜è®­ç»ƒæ•ˆç‡, [ç»éªŒå›æ”¾ é‡è¦æ€§é‡‡æ · ä¼˜å…ˆçº§é‡‡æ ·]
- ç¨³å®šæ€§æå‡
 - æé«˜è®­ç»ƒç¨³å®šæ€§,[æ¢¯åº¦ç´¯ç§¯ å­¦ä¹ ç‡è°ƒåº¦ æ—©åœç­–ç•¥]

#### å…³é”®æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ
- å¥–åŠ±åå·®: 
    - é—®é¢˜: å¥–åŠ±æ¨¡å‹å¯èƒ½å­˜åœ¨åè§,
    - è§£å†³: [å¤šæ ·åŒ–æ•°æ®, å…¬å¹³æ€§çº¦æŸ, äººç±»æ ¡éªŒ]

- æ¢ç´¢æ•ˆç‡: 
    - é—®é¢˜: ç­–ç•¥æœç´¢ç©ºé—´å·¨å¤§,
    - è§£å†³: [å¼•å¯¼æ¢ç´¢, è¯¾ç¨‹å­¦ä¹ , åˆ†å±‚å¼ºåŒ–å­¦ä¹ ]

- è®¡ç®—æˆæœ¬: 
    - é—®é¢˜: è®­ç»ƒèµ„æºæ¶ˆè€—å¤§,
    - è§£å†³: [æ¨¡å‹è’¸é¦, å‚æ•°é«˜æ•ˆå¾®è°ƒ, åˆ†å¸ƒå¼è®­ç»ƒ]

#### è¯„ä¼°å’Œç›‘æ§
- è®­ç»ƒç›‘æ§:
  - loss_tracking: å„ç»„ä»¶æŸå¤±å‡½æ•°
  - reward_stats: å¥–åŠ±åˆ†å¸ƒç»Ÿè®¡
  - policy_metrics: ç­–ç•¥å˜åŒ–æŒ‡æ ‡

 - æ€§èƒ½è¯„ä¼°
   -  human_eval: äººç±»è¯„ä¼°ç»“æœ
   -  automated_metrics: è‡ªåŠ¨åŒ–æŒ‡æ ‡
   -  behavior_analysis: è¡Œä¸ºåˆ†æ


### reward_approaches
- PPO
  - ç‰¹ç‚¹: ä½¿ç”¨ç½®ä¿¡åŒºé—´é™åˆ¶ç­–ç•¥æ›´æ–°
  - ä¼˜åŠ¿:
  - ä¸RMå…³ç³»: ä½¿ç”¨RMæä¾›å¥–åŠ±ä¿¡å·"
- DPO
  - ç‰¹ç‚¹:ç›´æ¥ä¼˜åŒ–ç­–ç•¥å·®å¼‚
  - ä¼˜åŠ¿:ä¸éœ€è¦æ˜¾å¼çš„RM,éšå¼å­¦ä¹ å¥–åŠ±
- RLHF+PPO

#### æ•°æ®æ”¶é›†æ–¹æ³•
- ç›´æ¥ç”Ÿæˆ: è®©æ¨¡å‹å¯¹åŒä¸€promptç”Ÿæˆå¤šä¸ªå›ç­”
- æ¸©åº¦é‡‡æ ·: ä½¿ç”¨ä¸åŒæ¸©åº¦å‚æ•°ç”Ÿæˆå¤šæ ·åŒ–è¾“å‡º
- äººå·¥ç¼–å†™: äººå·¥åˆ›ä½œå¤šä¸ªä¸åŒè´¨é‡çš„å›ç­”
- çœŸå®æ•°æ®: æ”¶é›†çœŸå®äººç±»å¯¹è¯/å†™ä½œæ ·æœ¬
#### æ„å»ºåå¥½å¯¹
#### æ ‡æ³¨ç­–ç•¥
- äºŒå…ƒæ¯”è¾ƒ: ç›´æ¥é€‰æ‹©å“ªä¸ªæ›´å¥½
- æå…‹ç‰¹é‡è¡¨: 1-5åˆ†æ‰“åˆ†
- ç›¸å¯¹æ’åº: å¤šä¸ªç­”æ¡ˆæ’åº
- æ¡ä»¶è¯„ä¼°: æ ¹æ®ç‰¹å®šæ ‡å‡†è¯„åˆ†

#### è®­ç»ƒè¿‡ç¨‹



åŸç†ä¸æ–¹æ³•
A. æ•°æ®æ”¶é›†
å¯¹åŒä¸€ä¸ªè¾“å…¥xï¼Œæ”¶é›†ä¸¤ä¸ªä¸åŒçš„è¾“å‡ºy1, y2  
è®©äººç±»æ ‡æ³¨è€…åˆ¤æ–­å“ªä¸ªè¾“å‡ºæ›´å¥½  
æ„å»ºåå¥½å¯¹(preferred, non-preferred)  

B. è®­ç»ƒè¿‡ç¨‹
å°†äººç±»çš„åå¥½åˆ¤æ–­è½¬åŒ–ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜  
ä½¿ç”¨æˆå¯¹æ¯”è¾ƒ(paired comparison)æ–¹æ³•  
ç›®æ ‡æ˜¯è®©æ¨¡å‹å¯¹è¾ƒå¥½çš„è¾“å‡ºèµ‹äºˆæ›´é«˜çš„å¥–åŠ±å€¼  

C. å…³é”®æ€è·¯
```python
# å‡è®¾æœ‰è¾“å…¥xå’Œä¸¤ä¸ªè¾“å‡ºy_w(better), y_l(worse)
reward_better = r_theta(x, y_w)  # é¢„æµ‹betteræ ·æœ¬çš„å¥–åŠ±
reward_worse = r_theta(x, y_l)   # é¢„æµ‹worseæ ·æœ¬çš„å¥–åŠ±
delta = reward_better - reward_worse  # è®¡ç®—å·®å€¼
prob = sigmoid(delta)  # è½¬æ¢ä¸ºèƒœç‡
```

Î´ = r_w - r_l = log odds = log(p/(1-p))
r_w: è¾ƒå¥½è¾“å‡ºçš„å¥–åŠ±å€¼  
r_l: è¾ƒå·®è¾“å‡ºçš„å¥–åŠ±å€¼  
Î´: å¥–åŠ±å·®å€¼(logit)  
p/(1-p): ä¼˜èƒœå‡ ç‡(odds)  
Ïƒ(Î´) è¡¨ç¤º sigmoid å‡½æ•°  

> ä¼˜èƒœå‡ ç‡(odds)
> å¦‚ p = 0.8ï¼Œè¡¨ç¤º80%çš„æ¦‚ç‡è·èƒœ  
> åˆ™ 1-p = 0.2ï¼Œè¡¨ç¤º20%çš„æ¦‚ç‡å¤±è´¥  
> odds = 0.8/0.2 = 4ï¼Œè¡¨ç¤º"èµ¢/è¾“"çš„æ¯”ç‡ä¸º4:1  


![image](https://github.com/user-attachments/assets/c6609699-0a92-4156-a0bc-bf8d14b29501)


![image](https://github.com/user-attachments/assets/8aa5b5a9-1e1d-42fc-8612-7ff94b0705fa)
- C(2,K)æ˜¯ç»„åˆæ•°ï¼Œè¡¨ç¤ºä»Kä¸ªæ ·æœ¬ä¸­å–2ä¸ªçš„ç»„åˆæ•°,å½’ä¸€åŒ–ç³»æ•°
- rÎ¸æ˜¯å¥–åŠ±æ¨¡å‹.xæ˜¯è¾“å…¥,ywæ˜¯è¾ƒå¥½çš„è¾“å‡º,ylæ˜¯è¾ƒå·®çš„è¾“å‡º,è®¡ç®—å¥–åŠ±å·®å€¼
- 

```python
def compute_loss(reward_model, x, y_better, y_worse):
    # è®¡ç®—å¥–åŠ±å€¼
    reward_better = reward_model(x, y_better)
    reward_worse = reward_model(x, y_worse)
    
    # è®¡ç®—å¥–åŠ±å·®å€¼
    delta = reward_better - reward_worse
    
    # é€šè¿‡sigmoidè½¬æ¢ä¸ºæ¦‚ç‡
    prob = torch.sigmoid(delta)
    
    # è®¡ç®—è´Ÿlog likelihood
    loss = -torch.log(prob)
    
    return loss.mean()
```


# Model Quantization

![image](https://github.com/hinswhale/AI-Learning/assets/22999866/785ee7cb-5f90-4b12-9879-fdaa2a1caeac)


# 量化 Model Quantization

概念：`量化`，`反量化`，`量化误差`

将连续的输入映射到离散的输出集合。

## **量化映射**

- Fixed Point Approximation
    
    定点近似主要是缩小浮点表示中的指数和小数部分的位宽，不需要额外的量化参数，也没有反量化过程，实现相对简单，但是在数值较大时，直接定点近似会带来较大的精度损失。
    
    !https://robot9.me/wp-content/uploads/2023/12/p5.png
    
- Range Based Approximation
    
    基于范围的近似，则是先统计待量化数据的分布，然后进行整体的缩放和偏移，再映射到量化空间，精度相对更高，但需要额外存储量化参数(如缩放系数、偏移等)，并且计算时需要先反量化，比定点近似更复杂。根据映射方式的不同，又可分为线性映射和非线性映射：
    

### **`量化`**

**`量化公式`**：$x_q = \text{round}\left(\frac{x}{\text{S}}\right) + \text{Z}$  (将浮点数向量 𝒙 转化为量化值 𝒙𝒒)

𝑆 表示缩放因子，用于确定裁剪范围

𝑍 表示零点因子，用于确定对称或非对称量化，也就是 fp32 中的零在量化 tensor 中的值。

round(·)表示将缩放后的浮点值映射为近似整数的取整操作。

### **`反量化`**

**`反量化`**（Dequantization）：从量化值中恢复原始值

$\tilde{x} = (x_q - \text{Z}) \times \text{S}$

`量化误差`是原始值 𝒙 和恢复值$\tilde{x}$之间的数值差异： $\Delta = \left\| \mathbf{x} - \mathbf{\tilde{x}} \right\|_2^2$

S 是输入范围与输出范围的比例： $\text{S} = \frac{\text{max} - \text{min}}{q_{\text{max}} - q_{\text{min}}}$ 

其中$[ \text{min}, \text{max} ]$是输入的裁剪范围，即允许输入的边界。 $[ q_{\text{min}}, q_{\text{max}} ]$是量化输出空间中的范围。

一般来说，裁剪范围对于量化性能有很大影响，通常需要根据实际数据分布进行**校准**，可以通过**`*静态（离线）***
或***动态（运行时）方式***`。

### 数值范围计算

- Min-Max：使用校准期间的最大绝对值；
- Histogram：将范围设置为校准期间看到的绝对值分布的百分位。
- Entropy：使用 KL 散度来最小化原始浮点值和量化比特表示的值之间的信息损失

## 量化计算

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/c8e348c4-de74-4228-9bcf-5480b5f17454/Untitled.png)

- q、z 都是整型计算，  S 为浮点型

将浮点数用 Q 表示法转换为定点数：

- $q_{fixed} = (int) (x_{float} * 2^Q)$
- $x_{float} = (float) (q_{fixed} * 2^{-Q})$
    
    

则两个浮点数 x1、x2 的乘法：

- $x_1*x_2 = (q_1*2^{-Q}) (q_2*2^{-Q}) = (q_1*q_2) 2^{-2Q} = ((q_1* q_2) >> Q)* 2^{-Q}$

整型的乘法(q1 * q2)和移位操作

最终只有整型的乘法(q1 * q2)和移位操作，举个例子：

- 0.234 * 0.84 = 0.19656，取 Q = 30
- 0.234 ≈ 251255586 * 2 ^-30
- 0.84 ≈ 901943132 * 2 ^-30
- 251255586 * 901943132 = 226618250169335352 (注意这里需要使用64位乘法，避免溢出)
- 226618250169335352 >> 30 = 211054692

即计算结果为： 211054692 * *2^(-30) ，转换为浮点数：211054692* * *2^(-30)*= 0.19655999913 ，

## **量化的对象**

- **权重**（weight）：量化weight可达到减少模型大小内存和占用空间。 如：W4A16、AWQ及GPTQ中的W4A16，W8A16（权重量化为INT8，激活仍为BF16或FP16）
- **激活**（activation）：**实际中activation往往是占内存使用的大头。结合权重量化一起使用，**如：SmoothQuant中的W8A8
- **KV cache**：**提高长序列生成的吞吐量**
- **梯度**（Gradients）：主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，同时，也可以减少backward时的开销。

## 量化策略

### 1. 均匀量化和非均匀量化

均匀量化：在量化过程中，量化函数产生的量化值之间的间距是均匀分布的。 使用更广泛

### 2. 对称量化和非对称量化

根据零点因子 𝑍 是否为零

对称量化：原始输入数据中的零点（𝑥 = 0）在量化后仍然对应到整数区间的零点

非对称量化：整数区间的零点对应到输入数值的 𝑆 · 𝑍

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/a89b7878-9e67-4ed9-a733-daaaf5da60fd/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/2b5eed05-3655-4f5b-a8af-2bc465df7120/Untitled.png)

对称量化 1. 会导致大量整型数值的浪费 2.  由于覆盖的范围更大，对称量化会引入更大的量化误差。

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/636d0797-4594-4abf-9596-8ee69a96e5a6/Untitled.png)

### 3. 线性 / 非线性

非线性：

非线性对数据分布密集的区域，给与更多的量化映射，就能增加量化后的差异性，提高精度。实际上，我们希望量化后的数据在量化空间应该均匀分布，而不是被原始数据的分布所影响。

如：分位量化方法（Quantile Quantization）

- 寻找一些分位点对原数据进行划分，使得各个区间的数据个数相等，然后将同一个区间的数据映射到同一个值，从而实现了量化
- NF4(4-bit NormalFloat Quantization) 是分位量化的一种实现

### 4. 量化粒度的选择

量化算法通常针对一个批次的数据进行处理，其中批次的规模大小就反应了量化粒度，

逐层量化（per-tensor）： 每层使用一套量化因子

逐通道（per-token & per-channel 或者 vector-wise quantization ）量化：按channel划分量化因子

逐组量化（per-group、Group-wise）：在每层中按照group使用一套量化因子

- 粒度越大，精度损失越大
- 量化粒度越小，需要额外存储的量化系数就越多

量化粒度有多种形式：

- per-tensor/per-layer
- per-channel/per-axis
- per-col/per-row
- per-embeding/per-token
- per-block/group

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/3798c953-e1ef-48ea-b893-3a3d0280e069/Untitled.png)

### **5. 精度选择**

**统一精度**

**混合精度**

## 量化方法

- 权重：训练完后固定，数值范围(range)与输入无关，可离线完成量化，通常相对容易量化；
- 激活：激活输出随输入变化而变化，需要统计数据动态范围，通常更难量化。范围统计的时机有两种：
    - training：训练时进行统计
    - calibration：训练后推理小批量数据进行统计

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/c28ec7bc-0a1e-40a9-85a2-719a9fac702e/Untitled.png)

---

### **量化感知训练Quantization Aware Training（QAT）**

- 训练时量化，伪量化，在线量化（在模型中添加伪量化节点模拟量化，重新训练模型（finetune），流程相对复杂）
- 开销较大，数据要求高
- 流程：
    1. 在训练好的模型上设置权重和激活值的min-max范围的初始值，插入伪量化算子（对数值量化然后反量化），模拟量化产生的误差
    2. 在训练数据集更新权重并调整对应的量化参数(zero-point、scale)，或者直接将量化参数作为可学习的参数在反向传播中更新。

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/0c746037-58d5-4131-ba35-88d516a625c3/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/13412dda-0ba2-4b54-8928-eaaf7a7fd064/Untitled.png)

forward pass 过程中，伪量化节点 fq 的量化是一个阶梯型的函数，而在 backward pass 中，为了反向传播能正常进行，将 fq 的梯度计算设置为直通（STE），这样在 QAT 训练中，loss 就带上了量化影响，并且能正常进行梯度下降权重更新。下图是算子 QAT 过程的流程示意图

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/0a5635ba-a477-4799-933e-b6fb207c1405/Untitled.png)

---

### **训练后量化 Post Training Quantization （PTQ）**

> tips: 激活值中存在较大的数值
> 

- 相对简单高效，已训练好的模型 + 少量校准数据，无需重新训练模型，根据是否量化激活又分为：
- **权重量化**
    - 仅量化权重，激活在推理时量化
    - 仅量化模型的权重以压缩模型的大小，在推理时将权重反量化为原始的float32数据，后续推理流程与普通的float32模型一致，因此推理性能不会提高
    - 不需要校准数据集，不需要实现量化算子，模型的精度误差较小
- **全量化**
    - 模型权重 + 激活值
    - 在模型推理时执行量化算子来加快模型的推理速度
    - 需要提供一定数量的校准数据集用于统计每一层激活值的分布，并对量化后的算子做校准
    - **分类：**
        - **Dynamic Quantization**  ：仅量化权重，激活值相关的量化参数是在推理阶段实时计算的，无需校准数据
        - **Static Quantization** 静态量化
            - **权重 + 模型的激活值**
            - 离线计算好模型权重和激活的量化参数，推理的时候不再调整直接使用
            - 需要一定数量的校准数据集用于统计每一层激活值的分布，并对量化后的算子做校准

- **Dynamic Quantization**
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/84c94734-5644-4bff-941e-18fce2974022/Untitled.png)
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/42828733-7288-4096-9bd5-1b229a901937/Untitled.png)

- **Static Quantization**
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/3266ef28-4e2a-4342-b2d6-33aadb229c49/Untitled.png)
    
- **校正量化误差
保证量化后的分布和量化前是一致的**

---

激活的数值范围统计：

- Min-Max
    
    直接统计最小、最大值：
    
    !https://robot9.me/wp-content/uploads/2023/12/p27.png
    
- Moving Average Min-Max
    
    增加了滑动平均，即当前的 min、max 与历史 min、max 的加权和
    
    !https://robot9.me/wp-content/uploads/2023/12/p27_2.png
    
- Histogram
    
    统计数值分布直方图，然后基于最小化量化误差或 KL 散度等算法选择合适的范围，如下图，选择 [0, 1.5] 区间就能覆盖 99% 以上的值
    

![截屏2024-05-21 21.35.50.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/8009437d-1dc0-4ca8-becb-189bc1b1bb27/%E6%88%AA%E5%B1%8F2024-05-21_21.35.50.png)

# Model Quantization

数据量化：

权重量化（也称为模型参数量化）和激活（值）量化，

## 量化

概念：`量化`，`反量化`，`量化误差`

将连续的输入映射到离散的输出集合。

将浮点数向量 𝒙 转化为量化值 𝒙𝒒

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/5e1062bd-4cfd-436b-ac41-e65428ff26a0/Untitled.png)

𝑆 表示缩放因子，用于确定裁剪范围

𝑍 表示零点因子，用于确定对称或非对称量化，

 𝑅(·)表示将缩放后的浮点值映射为近似整数的取整操作。

一般来说，裁剪范围对于量化性能有很大影响，通常需要根据实际数据分布进行**校准**，可以通过*`**静态（离线）***
或***动态（运行时）方式***`。

`**反量化**`（Dequantization）：从量化值中恢复原始值

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/dcf51e47-b21c-419a-a563-99871e18ad3a/Untitled.png)

`量化误差`是原始值 𝒙 和恢复值$\tilde{x}$之间的数值差异： $\Delta = \left\| \mathbf{x} - \mathbf{\tilde{x}} \right\|_2^2$

## 量化策略

### 均匀量化和非均匀量化

均匀量化：在量化过程中，量化函数产生的量化值之间的间距是均匀分布的。 使用更广泛

### 对称量化和非对称量化

根据零点因子 𝑍 是否为零

对称量化：原始输入数据中的零点（𝑥 = 0）在量化后仍然对应到整数区间的零点

非对称量化：整数区间的零点对应到输入数值的 𝑆 · 𝑍

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/ff7e5421-fba1-4fda-95e0-7ff1d9b5994f/Untitled.png)

对称量化 1. 会导致大量整型数值的浪费 2.  由于覆盖的范围更大，对称量化会引入更大的量化误差。

### 量化粒度的选择

量化算法通常针对一个批次的数据进行处理，其中批次的规模大小就反应了量化粒度，

按张量量化：每个张量只定义一组量化参数

按通道量化

按组量化

## 量化方法

量化感知训练（Quantization-Aware Training, QAT）

训练后量化（Post-Training Quantization, PTQ）

QAT 需要更新权重

激活值中存在较大的数值

### 权重量化

逐层量化：最小化逐层的重构损失

量化权重，

优化目标：
$\arg \min_{\hat{W}} \|XW - \hat{X}\hat{W}\|_2^2$

其中 $W, \hat{W}$分别表示原始权重和量化后的权重，X为输入。

GPTQ： 逐层量化 + 权重矩阵按照列维度分组

每列参数量化后，需要适当调整组内其他未量化的参数，以弥补当前量化造成的精度损失

设计的优化方法： 延迟批次更新、 Cholesky 重构

AWQ：逐层和逐组权重量化 +  关注那些与较大激活值维度相对应的关键权重

引入针对权重的激活感知缩放策略，使得量化方法更为针对性地处理关键权重所对应的权重维度

优化目标：

$\|( \mathrm{diag}(s)^{-1} \cdot X ) \cdot Q(W \cdot \mathrm{diag}(s)) - XW \|_2^2，$

其中 Q为量化函数。通过引入缩放因子 s，

### 权重和激活值量化

细粒度量化 ：

ZeroQuant

混合精度分解：【异常值涌现现象】

将具有异常值的特征维度与其他正常维度分开计算。

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/61b4b868-00a0-4180-ba5f-2b9ce2d59ad3/Untitled.png)

量化难度平衡

SmoothQuant

高效微调增强量化：

QLoRA： 小型可调适配器 + 用 16 比特精度进行训练与学习

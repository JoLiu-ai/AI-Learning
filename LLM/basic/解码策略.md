## 策略

### 搜索策略

- 贪心搜索（Greedy Search）
    - 直接选择分布中概率最大的token当作解码出来的词
    - 在机器翻译和文本摘要等任务中，任务输出高度依赖于输入内容，适合用贪心搜索，但是在开放式生成任务（如故事生成和对话系统）中，贪心搜索有时会因为过于关注局部最优，而生成不自然、重复的句子

  ![image](https://github.com/hinswhale/AI-Learning/assets/22999866/3752c4e5-7e2b-4cf7-9a3e-cae1812a5071)

    
- Beam Search
    - 保留前 𝑛 个具有最高概率的句子，并最终选取整体概率最高的生成回复。当 𝑛 = 1，束搜索就退化为贪心搜索
![image](https://github.com/hinswhale/AI-Learning/assets/22999866/04094597-cbf6-4804-b3ed-863d36e28496)

    
- 长度惩罚/长度归一化 Length Penalty
    
    如果没有长度惩罚，传统的束搜索会倾向于生成较短的句子
    
    通过将句子概率除以其长度的指数幂 𝛼，对于句子概率进行归一化处理，从而鼓励模型生成更长的句子。在实践中，𝛼 通常设置为 0.6 到 0.7 之间的数值。
    
- 重复惩罚
    - 𝑛-元惩罚（𝑛-gram Penalty） 实践中 𝑛 通常设置为 3 到 5 之间的整数。
    - 出现惩罚（Presence Penalty）
        
        将已经生成词元的 logits减去惩罚项 𝛼 
        
    - 频率惩罚（Frequency Penalty）
        
        每个词元生成的数目，然后减去出现次数乘以惩罚项 𝛼
        

### 概率采样（Probability Sampling）

根据模型建模的概率分布采样得到下一个词元

- Temperature Sampling
    - 通过温度，在采样前调整每个词的概率分布
    - temperature(取值范围：0-1)设的越高，生成文本的自由创作空间越大，更具多样性。温度越低，生成的文本越偏保守，更稳定。
    - 一般来说，prompt 越长，描述得越清楚，模型生成的输出质量就越好，置信度越高，这时可以适当调高 temperature 的值；反过来，如果 prompt 很短，很含糊，这时再设置一个比较高的 temperature 值，模型的输出就很不稳定了。

 ![image](https://github.com/hinswhale/AI-Learning/assets/22999866/9d12455b-3b7b-4cc7-a373-2a93ce640d86)

        
- Top-𝑘 Sampling
    - 从根据tokens的 likelihood scores 来采样模型从最可能的"k"个选项中随机选择一个
    - 缺点：不考虑整体概率分布。**在分布陡峭的时候仍会采样到概率小的单词，或者在分布平缓的时候只能采样到部分可用单词**
- Top-𝑝 Sampling
    - **模型从累计概率大于或等于“p”的最小集合中随机选择一个**
    - top-P越高，候选词越多，多样性越丰富。top-P越低，候选词越少，越稳定
- Contrastive Decoding
    - 通过计算一个较大的语言模型和一个较小的语言模型之间的对数概率分布差值，然后基于归一化的差值分布采样下一个词元，从而有效地提升重要词元在生成过程中的影响力

在翻译和摘要任务：侧重 `搜索策略`

生成任务：侧重 `采样策略`


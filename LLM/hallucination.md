### what is hallucination
- faithfulness 输出的信息可能不相关或偏离用户的问题
- factualness 生成的信息与已知事实不符（例如历史、科学数据等）
- 模型可能会创造不存在的事件、人物或细节

### 具体体现
- 错误的事实：模型可能会生成不准确、过时或完全错误的事实。例如，提供一个虚假的历史事件，或者错误地列举某个人物的贡献。
- 不存在的引用：有时模型会引用一些“伪造的”文献、研究或文章，尽管这些引用并不存在于真实世界中。
- 矛盾的回答：在多轮对话中，模型可能会给出相互矛盾的回答，或者提供不一致的信息。
- 超出能力的推理：当模型面临无法推理或模糊的输入时，可能会“填补空白”，生成无法验证的内容。
- 无关内容的生成：在某些情况下，模型可能会生成与输入问题或对话主题无关的内容，或者内容中包含不必要的冗余信息。


### why hallucination
- 训练数据的缺陷
  - 训练数据中的偏差和噪声（重复数据/可信度）
  - 缺乏代表性数据
- 缺乏外部知识验证（没有实时访问互联网或数据库的情况下进行推理的）
- 模型架构的局限性
  -  模型结构
  -  解码算法
  -  训练和测试阶段不匹配的 exposure bias 问题
  -  训练时记忆错误信息
- 上下文理解的不足

### 如何评估
【todo】

### 如何减少hallucination
- 数据集
 - 人工标注/评测数据
 - 利用模型筛除不良数据
 - 可靠数据集加权
- 模型层
  - 编码更多信息
  - 减少随机性
- RAG

#### 参考方案
- 减少随机性，增加可控文本生成
- 规划框架再执行模型生成
- 强化学习
- 多任务学习
- 后处理： 识别并修正模型输出中的幻觉错误

### AI集群硬件与通信技术对比

| **分类**       | **技术**   | **主要用途**                                                                 | **适用场景**       | **补充说明**                                                                 |
|----------------|------------|-----------------------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------|
| **硬件层面**   | PCIe       | 连接CPU与GPU，负责基础数据传输（如内存到显存）                                | 通用计算场景       | 最新PCIe 6.0带宽达256 GB/s（x16双向），支持CXL协议实现异构内存池化          |
|                | NVLink     | GPU间高速互联，支持点对点通信和共享显存                                      | 单机多卡           | 第四代NVLink单链路带宽112.5 GB/s，全互联需NVSwitch支持（如DGX系统）         |
|                | RDMA       | 跨节点GPU直接内存访问，绕过CPU和操作系统                                     | 多机多卡           | 协议对比：<br>- InfiniBand：性能最优，需专用网络<br>- RoCE：兼容以太网<br>- iWARP：支持TCP/IP |
| **通信层面**   | NCCL       | NVIDIA优化的多GPU集合通信库（如AllReduce、Broadcast）                        | 多GPU节点          | 自动选择NVLink/PCIe最优路径，比传统MPI快3-5倍                              |
|                | MPI        | 跨节点进程管理与消息传递（Send/Recv），传统HPC标准                           | CPU集群            | 常与NCCL协同：MPI管理跨节点，NCCL处理节点内GPU通信                          |

#### 关键协作关系
1. **单机场景**：NVLink + NCCL（GPU间） + PCIe（CPU-GPU）
2. **多机场景**：RDMA（跨节点） + MPI（进程协调） + NCCL（单机GPU优化）

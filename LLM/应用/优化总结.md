###  vLLM 
- Page attention
  - Inspiration from operating systems (OS): virtual memory and paging
- share prompt

###  Flashattention 
- kernel fusion
- avoid using the slow HBM

###  speculative decoding
- the generation of the token is memory bounded (especially at a small batch size).
- method: run two models in parallel.
  - Target Model: The main LLM we want to use for our task.
  - Small Draft Model: A smaller, lightweight LLM that runs alongside to help speed up the main LLM’s inference process.
- Procedure:
   - The draft model decodes K tokens autoregressively
   - Feed the K generated tokens in parallel into the target model and get the predicted probabilities on each location
   - Decide if we want to keep the K tokens or reject them   

![image](https://github.com/user-attachments/assets/be4eb8af-43c4-4eda-bef3-f8d0a014402e)

✔︎ **no-autoregressive model** vs **autoregressive model**  
![image](https://github.com/user-attachments/assets/5c3c8eba-c0c4-48cd-8270-46ae7a4eee50)

✔︎ **compressed model**

###  Batching
- No Batching
- Static Batching
- Dynamic Batching
- Continuous Batching

### Quantization
- INT4/INT8 Weight-Only Quantization (W4A16 & W8A16)
- SmoothQuant
- GPTQ
-  AWQ
- FP8

### Parallelism
- Tensor Parallelism
- Pipeline Parallelism


### Attention
- Multi-head Attention (MHA)   
- Multi-query Attention (MQA)   
- Group-query Attention (GQA)   

### decoding
- Greedy-search   
- Beam-search  
- ROPE  

###
1/ EfficientML.ai Lecture 13

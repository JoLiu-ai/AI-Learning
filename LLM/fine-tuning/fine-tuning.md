processingï¼šæœªå®Œæˆ

### Prompt Tuning
- add **soft prompt**ï¼šä½¿ç”¨ä¸€ç»„ æ‰‹åŠ¨è®¾è®¡çš„æç¤º æ¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡
- é€šè¿‡å¼•å…¥ è™šæ‹Ÿ tokenï¼ˆé€šå¸¸æ˜¯â€œåµŒå…¥çš„æç¤ºâ€ï¼‰åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œè¿™äº›è™šæ‹Ÿ token ç”¨äºå¼•å¯¼æ¨¡å‹è¿›è¡Œæ›´åˆé€‚çš„ç”Ÿæˆæˆ–æ¨ç†ä»»åŠ¡ã€‚æ•´ä¸ªæ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ï¼ˆå¦‚ transformer å±‚çš„æƒé‡ï¼‰ä¿æŒä¸å˜ï¼Œä»…é€šè¿‡å­¦ä¹ è¿™äº›è™šæ‹Ÿ token çš„åµŒå…¥æ¥è°ƒæ•´æ¨¡å‹çš„è¡Œä¸º  
```[è™šæ‹Ÿ token 1] + [è™šæ‹Ÿ token 2] + ... + "æ–‡æœ¬å†…å®¹" ```

- å­¦ä¹ ç»“æ„åŒ–çš„æç¤º token åµŒå…¥
- è¾“å…¥çš„æç¤ºå›ºå®šé•¿åº¦ï¼šPrompt Tuning åœ¨è¾“å…¥æ–‡æœ¬ä¸­åŠ å…¥ä¸€ä¸ªå›ºå®šé•¿åº¦çš„æç¤ºï¼ˆè™šæ‹Ÿ token çš„åµŒå…¥ï¼‰ï¼Œé€šè¿‡å¾®è°ƒè¿™äº›åµŒå…¥æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
- åµŒå…¥çš„ä½ç½®ï¼šé€šå¸¸æ˜¯è¾“å…¥æ–‡æœ¬çš„å‰æˆ–åéƒ¨åˆ†
- å­¦ä¹ å›ºå®šé•¿åº¦çš„è½¯æç¤ºæ ‡è®°

ä¼ªä»£ç 
```python
# å­¦ä¹ å¯è®­ç»ƒçš„è½¯æç¤º
soft_prompt = nn.Parameter(torch.randn(num_tokens, embedding_dim))
input_embeddings = torch.cat([soft_prompt, original_embeddings], dim=0)
```

#### ä»£ç 
é…ç½®
```python
peft_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    prompt_tuning_init=PromptTuningInit.TEXT,
    num_virtual_tokens=8,
    prompt_tuning_init_text="Classify if the tweet is a complaint or not:",
    tokenizer_name_or_path=model_name_or_path,
)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```

**reasoning**  
```python
from peft import PeftModel, PeftConfig

peft_model_id = f"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}"

# åŠ è½½PEFTé…ç½®
config = PeftConfig.from_pretrained(peft_model_id)

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
# åŠ è½½PEFTæ¨¡å‹
model = PeftModel.from_pretrained(model, peft_model_id)

# Tokenizerç¼–ç 
inputs = tokenizer(f'{text_column} : {dataset["test"][i]["Tweet text"]} Label : ', return_tensors="pt")

# æ¨¡å‹æ¨ç†
outputs = model.generate(
        input_ids=inputs["input_ids"], 
        attention_mask=inputs["attention_mask"], 
        max_new_tokens=10, 
        eos_token_id=3
    )

# Tokenizer è§£ç 
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))
```


#### æºç 

```python
class PromptEmbedding(torch.nn.Module):
    def __init__(self, config, word_embeddings):
        super().__init__()

        total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules
        self.embedding = torch.nn.Embedding(total_virtual_tokens, config.token_dim)
        
        if config.prompt_tuning_init == PromptTuningInit.TEXT:
            from transformers import AutoTokenizer

            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)
            init_text = config.prompt_tuning_init_text
            init_token_ids = tokenizer(init_text)["input_ids"]
            
            num_text_tokens = len(init_token_ids)
            if num_text_tokens > total_virtual_tokens:
                init_token_ids = init_token_ids[:total_virtual_tokens]
            elif num_text_tokens < total_virtual_tokens:
                num_reps = math.ceil(total_virtual_tokens / num_text_tokens)
                init_token_ids = init_token_ids * num_reps
            init_token_ids = init_token_ids[:total_virtual_tokens]

            word_embedding_weights = word_embeddings(torch.LongTensor(init_token_ids)).detach().clone()
            word_embedding_weights = word_embedding_weights.to(torch.float32)
            self.embedding.weight = torch.nn.Parameter(word_embedding_weights)

    def forward(self, indices):
        # è¿™é‡Œçš„ indices æ˜¯è™šæ‹Ÿ token çš„ç´¢å¼•
        prompt_embeddings = self.embedding(indices)
        return prompt_embeddings
```

### P-Tuning
- å¯å¾®çš„virtual tokenï¼Œä½†ä»…é™äºè¾“å…¥å±‚ï¼Œæ²¡æœ‰åœ¨æ¯ä¸€å±‚éƒ½åŠ 
- ä½ç½®ä¹Ÿä¸ä¸€å®šæ˜¯å‰ç¼€ï¼Œæ’å…¥çš„ä½ç½®æ˜¯å¯é€‰çš„
- ä½¿ç”¨æç¤ºç¼–ç å™¨å¢å¼ºæç¤ºçš„è¯­ä¹‰è¡¨è¾¾

ä¼ªä»£ç 
```python
# ä½¿ç”¨LSTMç¼–ç æç¤º
prompt_encoder = nn.LSTM(...)
soft_prompt = prompt_encoder(learnable_tokens)
```

```python
peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=20, encoder_hidden_size=128)
```


#### æºç 
```python
class PromptEncoder(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_dim = config.token_dim
        self.input_size = self.token_dim
        self.output_size = self.token_dim
        self.hidden_size = config.encoder_hidden_size
        self.total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules
        self.encoder_type = config.encoder_reparameterization_type

        # åˆå§‹åŒ– embedding å±‚
        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)
        if not config.inference_mode:
            # æ ¹æ®PromptEncoderé‡å‚æ•°åŒ–ç±»å‹åˆå§‹åŒ–ç›¸åº”çš„lstmå’Œmlp
            if self.encoder_type == PromptEncoderReparameterizationType.LSTM:
                lstm_dropout = config.encoder_dropout
                num_layers = config.encoder_num_layers
                # LSTM
                self.lstm_head = torch.nn.LSTM(
                    input_size=self.input_size,
                    hidden_size=self.hidden_size,
                    num_layers=num_layers,
                    dropout=lstm_dropout,
                    bidirectional=True,
                    batch_first=True,
                )

                self.mlp_head = torch.nn.Sequential(
                    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),
                    torch.nn.ReLU(),
                    torch.nn.Linear(self.hidden_size * 2, self.output_size),
                )

            elif self.encoder_type == PromptEncoderReparameterizationType.MLP:
                warnings.warn(
                    f"for {self.encoder_type}, the `encoder_num_layers` is ignored. Exactly 2 MLP layers are used."
                )
                layers = [
                    torch.nn.Linear(self.input_size, self.hidden_size),
                    torch.nn.ReLU(),
                    torch.nn.Linear(self.hidden_size, self.hidden_size),
                    torch.nn.ReLU(),
                    torch.nn.Linear(self.hidden_size, self.output_size),
                ]
                self.mlp_head = torch.nn.Sequential(*layers)

            else:
                raise ValueError("Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.")
    def forward(self, indices):
        input_embeds = self.embedding(indices)
        if self.encoder_type == PromptEncoderReparameterizationType.LSTM:
            output_embeds = self.mlp_head(self.lstm_head(input_embeds)[0])
        elif self.encoder_type == PromptEncoderReparameterizationType.MLP:
            output_embeds = self.mlp_head(input_embeds)
        else:
            raise ValueError("Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.")

        return output_embeds
```

### Prefix Tuning
å…³é”®ç‰¹å¾æ˜¯ï¼š
- åœ¨æ¯ä¸€å±‚transformerå±‚çš„è¾“å…¥å‰æ·»åŠ å¯å­¦ä¹ çš„å‰ç¼€ï¼ˆprefixï¼‰
- å‰ç¼€æ˜¯è¿ç»­çš„å¯å­¦ä¹ å‘é‡
- å‰ç¼€è¢«æ·»åŠ åˆ°æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„é”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰å‘é‡ä¸­
- æ¨¡å‹å‚æ•°ä»ç„¶ä¿æŒå†»ç»“

ä¼ªä»£ç 
```python
# ä¸ºæ¯ä¸€å±‚å­¦ä¹ å‰ç¼€
prefix_keys = nn.Parameter(torch.randn(num_layers, num_heads, prefix_length, hidden_size))
prefix_values = nn.Parameter(torch.randn(num_layers, num_heads, prefix_length, hidden_size))
```

### Adapter Tuning
æ’å…¥å°‘é‡å¯è®­ç»ƒçš„å°å‹ç¥ç»ç½‘ç»œæ¨¡å—

### LoRA (Low-Rank Adaptation)
- åœ¨åŸå§‹æ¨¡å‹å‚æ•°æ—è¾¹æ·»åŠ ä½ç§©çŸ©é˜µ
- ä»…è®­ç»ƒè¿™äº›ä½ç§©çŸ©é˜µ
- å‚æ•°é‡æå°‘ï¼Œè®¡ç®—å¼€é”€å°

**å…¬å¼**ï¼š

LoRAï¼ˆLow-Rank Adaptationï¼‰é€šè¿‡å°†æ¨¡å‹æƒé‡ W åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µ A å’Œ B ï¼Œå¹¶åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶è®¡ç®—ä¸€ä¸ªä½ç§©è°ƒæ•´é¡¹ $$\Delta$$ W ï¼Œä»è€Œå¯¹åŸå§‹æƒé‡è¿›è¡Œè°ƒæ•´ã€‚

è°ƒæ•´åçš„æƒé‡è®¡ç®—å…¬å¼ä¸ºï¼š

$$
W' = W + \Delta W = W + \frac{\alpha}{r} \cdot A \cdot B
$$

å…¶ä¸­ï¼š
- W æ˜¯åŸå§‹çš„æ¨¡å‹æƒé‡çŸ©é˜µã€‚
- A å’Œ B æ˜¯ä½ç§©çŸ©é˜µï¼Œè¡¨ç¤ºå¯¹æƒé‡çŸ©é˜µ W çš„è°ƒæ•´ã€‚
- r æ˜¯ç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µçš„å¤§å°ã€‚
- $$\alpha$$ æ˜¯ LoRA ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´ä½ç§©çŸ©é˜µçš„å½±å“åŠ›ã€‚
- A $$\cdot$$ B æ˜¯ä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ï¼Œè¡¨ç¤ºå¯¹åŸå§‹æƒé‡çŸ©é˜µ W çš„å¢é‡ã€‚
å…¶ä¸­ï¼š
$ğ´ \in \mathbb{R}^{r \times d}$, $B \in \mathbb{R}^{r \times d}$æ˜¯ä½ç§©çŸ©é˜µ,r æ˜¯ç§©ï¼ˆrankï¼‰ï¼Œé€šå¸¸è®¾ç½®ä¸ºè¾ƒå°çš„å€¼ã€‚
è®­ç»ƒæ—¶ï¼Œä»…ä¼˜åŒ–ğ´å’Œ ğµï¼Œè€Œä¸æ›´æ–° ğ‘Š

> - Transformerçš„æƒé‡çŸ©é˜µåŒ…æ‹¬Attentionæ¨¡å—é‡Œç”¨äºè®¡ç®—query, key, valueçš„ $$W_q$$ ï¼Œ $$W_k$$ ï¼Œ $$W_v$$ ä»¥åŠå¤šå¤´attentionçš„ $$W_o$$ ,ä»¥åŠMLPå±‚çš„æƒé‡çŸ©é˜µ
> - LoRAåªåº”ç”¨äºAttentionæ¨¡å—ä¸­çš„4ç§æƒé‡çŸ©é˜µï¼Œè€Œä¸”é€šè¿‡æ¶ˆèå®éªŒå‘ç°åŒæ—¶è°ƒæ•´ $$W_q$$ å’Œ $$W_v$$ä¼šäº§ç”Ÿæœ€ä½³ç»“æœã€‚é»˜è®¤çš„æ¨¡å—ååŸºæœ¬éƒ½ä¸º $$W_q$$ å’Œ $$W_v$$ æƒé‡çŸ©é˜µã€‚


![image](https://github.com/user-attachments/assets/4bdfcf8c-c1c8-4e75-8a8c-0858d91f15ab)

```python
from peft import LoraConfig
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    inference_mode=False, 
    r=8, 
    lora_alpha=32, 
    lora_dropout=0.1
)
```
> - task_typeï¼šæŒ‡å®šä»»åŠ¡ç±»å‹ã€‚å¦‚ï¼š Causal Language Modelingï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼‰, SEQ2SEQ_LMï¼ˆåºåˆ—åˆ°åºåˆ—è¯­è¨€æ¨¡å‹ï¼Œå¦‚ T5ï¼‰ã€TOKEN_CLASSIFICATIONï¼ˆæ ‡æ³¨ä»»åŠ¡)ç­‰ã€‚  
> - inference_modeï¼šæ˜¯å¦åœ¨æ¨ç†æ¨¡å¼ä¸‹ä½¿ç”¨Peftæ¨¡å‹ã€‚  
> - rï¼š ä½ç§©åˆ†è§£ä¸­çŸ©é˜µğ´å’Œğµçš„ç§©,å¸¸ç”¨å€¼ï¼š4ã€8ã€16,è¶Šå¤§æ€§èƒ½è¶Šå¥½ï¼Œä½†å‚æ•°é‡å¢åŠ 
> - lora_alpha:LoRA çš„ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒèŠ‚ LoRA æƒé‡çš„å½±å“åŠ›,å…¸å‹èŒƒå›´ï¼š1-32.åœ¨åº”ç”¨ ğ´ å’Œ ğµ æ—¶ï¼Œæƒé‡è°ƒæ•´ä¸ºï¼šÎ”ğ‘Š=ğ›¼/ğ‘Ÿâ‹…(ğ´â‹…ğµ),å…¶ä¸­ ğ›¼=lora_alpha,è¿™å¯ä»¥æ”¾å¤§æˆ–ç¼©å° LoRA çš„è°ƒæ•´å¹…åº¦
> - lora_dropoutï¼šLoRA å±‚çš„ä¸¢å¼ƒï¼ˆdropoutï¼‰ç‡ï¼Œå–å€¼èŒƒå›´ä¸º[0, 1)ã€‚  
> - target_modulesï¼šè¦æ›¿æ¢ä¸º LoRA çš„æ¨¡å—åç§°åˆ—è¡¨æˆ–æ¨¡å—åç§°çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚é’ˆå¯¹ä¸åŒç±»å‹çš„æ¨¡å‹ï¼Œæ¨¡å—åç§°ä¸ä¸€æ ·ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å…·ä½“çš„æ¨¡å‹è¿›è¡Œè®¾ç½®ï¼Œæ¯”å¦‚ï¼ŒLLaMaçš„é»˜è®¤æ¨¡å—åä¸º[q_proj, v_proj]ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è‡ªè¡ŒæŒ‡å®šä¸ºï¼š[q_proj,k_proj,v_proj,o_proj]ã€‚ åœ¨ PEFT ä¸­æ”¯æŒçš„æ¨¡å‹é»˜è®¤çš„æ¨¡å—åå¦‚ä¸‹æ‰€ç¤ºï¼š

æºç 
```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4, alpha=1):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        
        # å†»ç»“åŸå§‹æƒé‡
        self.linear.weight.requires_grad = False
        
        # ä½ç§©çŸ©é˜µ
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # ç¼©æ”¾å› å­
        self.scaling = alpha / rank

    def forward(self, x):
        # åŸå§‹çº¿æ€§å˜æ¢
        base_output = self.linear(x)
        
        # LoRAæ›´æ–°
        lora_output = self.scaling * (x @ self.lora_A.T) @ self.lora_B.T
        
        return base_output + lora_output
```
å˜å½¢ï¼š
LoRA VS AdaLoRA vs QLoRA

| ç‰¹å¾     | LoRA | AdaLoRA | QLoRA |
|----------|------|---------|-------|
| å‚æ•°é€‚é… | å›ºå®šç§© | åŠ¨æ€ç§© | é‡åŒ– + ä½ç§© |
| è®¡ç®—æ•ˆç‡ | ä¸­ç­‰ | è¾ƒé«˜   | æœ€é«˜  |
| å‚æ•°é‡   | å°‘   | æ›´å°‘   | æœ€å°‘  |
| å¤æ‚åº¦   | ä½   | ä¸­ç­‰   | è¾ƒé«˜  |

- è®¡ç®—èµ„æºå……è¶³ï¼š
é€‰æ‹©AdaLoRA
æ€§èƒ½æœ€ä½³

- æ˜¾å­˜å—é™
é€‰æ‹©QLoRA
æä½å‚æ•°é‡

- å¿«é€ŸåŸå‹
é€‰æ‹©æ ‡å‡†LoRA
å®ç°æœ€ç®€å•

### **IA3**
- åŸç†ï¼šï¼ˆé€šè¿‡æŠ‘åˆ¶å’Œæ”¾å¤§å†…éƒ¨æ¿€æ´»æ³¨å…¥é€‚é…å™¨ï¼‰ä½¿ç”¨å­¦ä¹ å‘é‡é‡æ–°è°ƒæ•´å†…éƒ¨æ¿€æ´»
- æ¿€æ´»æ¨¡å—ï¼šåŸºäºtransformerçš„æ¶æ„ä¸­çš„attentionå’Œfeedforwardæ¨¡å—

### **P-Tuning ä¸ Prompt Tuning / Prefix Tuning çš„åŒºåˆ«**

| ç‰¹å¾                  | P-Tuning                           | Prompt Tuning                      | Prefix Tuning                    |
|-----------------------|------------------------------------|------------------------------------|----------------------------------|
| **æ ¸å¿ƒæ€æƒ³**           | å­¦ä¹ è™šæ‹Ÿ token çš„åµŒå…¥            | å­¦ä¹ ç»“æ„åŒ–çš„æç¤º token åµŒå…¥      | å‘è¾“å…¥åºåˆ—æ·»åŠ å¯å­¦ä¹ çš„å‰ç¼€åµŒå…¥ |
| **åµŒå…¥çš„ä½ç½®**         | è¾“å…¥æ–‡æœ¬çš„ä»»æ„ä½ç½®ï¼ˆå‰ã€åï¼‰     | é€šå¸¸æ˜¯è¾“å…¥æ–‡æœ¬çš„å‰æˆ–åéƒ¨åˆ†      | ä¸»è¦æ’å…¥åœ¨è¾“å…¥åºåˆ—çš„å‰ç¼€éƒ¨åˆ† |
| **å¾®è°ƒå†…å®¹**           | åªå¾®è°ƒè™šæ‹Ÿ token çš„åµŒå…¥         | å¾®è°ƒæç¤º token åµŒå…¥             | åªå¾®è°ƒå‰ç¼€åµŒå…¥ï¼Œä¸è°ƒæ•´æ¨¡å‹å…¶ä»–éƒ¨åˆ† |
| **ä¸è¾“å…¥çš„å…³ç³»**       | è™šæ‹Ÿ token åµŒå…¥ä¸æ–‡æœ¬æ‹¼æ¥       | æç¤º token ä¸æ–‡æœ¬æ‹¼æ¥            | å‰ç¼€åµŒå…¥ä¸æ–‡æœ¬æ‹¼æ¥             |
| **è®­ç»ƒæˆæœ¬**           | ä½ï¼ˆåªè°ƒæ•´å°‘é‡å‚æ•°ï¼‰            | ä½ï¼ˆåªè°ƒæ•´æç¤º token çš„åµŒå…¥ï¼‰  | ä½ï¼ˆåªè°ƒæ•´å‰ç¼€åµŒå…¥ï¼‰           |
| **åº”ç”¨åœºæ™¯**           | ç”Ÿæˆä»»åŠ¡ã€å°‘æ•°æ®å¾®è°ƒä»»åŠ¡        | ç”Ÿæˆä»»åŠ¡ã€æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡ | ç”Ÿæˆä»»åŠ¡ã€å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡       |

### å‚è€ƒèµ„æ–™
1. åƒæœå†»ä¸åæœå†»çš® https://zhuanlan.zhihu.com/p/649315197
2. 


### 📌 微调技术

#### 💡按Layer分组
- **Full Fine-Tuning**  
  对整个模型进行微调，更新所有参数。这种方法适用于任务专一且数据充足的场景，但计算成本高，可能导致过拟合。
  
- **Layer-wise Fine-Tuning**  
  微调时，只更新模型的某些层，而不更新其他层。这种方法通常适用于具有层次结构的模型，通过选择性地更新层，平衡性能和计算效率。

- **Freezing Layers**  
  冻结部分层，仅对其他层进行微调。通常在预训练模型的低层特征（如卷积层）冻结时，可以减少计算负担，特别是在数据量较少的情况下，适用于传递通用特征的情况。

#### 💡参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)
- **概念**  
  PEFT旨在减少需要微调的参数数量。通过只微调网络的部分参数（例如添加少量的附加模块，或利用低秩分解等方法），可以在保证性能的前提下，显著提高训练效率，减少计算资源和内存的需求。

#### 💡按任务分类
- **Multi-Task Fine-Tuning**  
  在一个模型上同时训练多个任务。不同于训练多个独立模型，这种方法能够利用共享的表示学习，从而提高任务间的迁移学习能力，尤其在资源有限的情况下尤为有效。

- **Domain-Adaptive Fine-Tuning**  
  将预训练模型调整到特定领域或数据分布上。通过针对领域特定数据的微调，可以让模型更好地适应目标领域中的特定语言、术语或任务模式。

- **Supervised Fine-Tuning**  
  使用带有标签的特定任务数据对模型进行微调。这要求大量的人工标注数据，且目标是通过有监督学习实现任务专一性。  
  **目标**：优化模型在特定任务上的性能。

- **Instruction Tuning**  
  向模型提供清晰的任务指令，使其能够处理各种任务。通过使用自然语言指令或任务描述，模型能够在无需单独训练每个任务的情况下执行多种任务。  
  **目标**：通过调整模型，使其具备处理不同任务的能力，提高多任务适应性。

- **Knowledge Distillation**  
  将大模型（教师模型）的知识“蒸馏”到小模型（学生模型），通过压缩模型来减少推理时的计算需求，提升推理效率，同时尽量保持原模型的性能。

- **Contrastive Fine-Tuning**  
  采用对比学习的策略进行微调，通常用于学习更加精细的区分特征，提升模型在相似样本中的判别能力。特别适用于图像检索、文本匹配等任务。

### 📌 微调方法分类

- **有监督微调 (Supervised Fine-Tuning, SFT)**  
  基于带标签数据的微调方法，旨在对预训练模型进行特定任务的优化。训练数据带有明确标签，模型通过最小化误差来优化权重，通常用于传统的分类或回归任务。

- **基于人类反馈的强化学习 (Reinforcement Learning with Human Feedback, RLHF)**  
  结合强化学习与人类反馈对模型进行微调。通过模拟环境与人类反馈交互，强化模型决策能力。这一方法常用于生成任务中，如对话生成和推荐系统等。
  - 强化学习（策略优化、回报最大化）
  - 方法：
    - Proximal Policy Optimization (PPO)
    - Trust Region Policy Optimization (TRPO)

### 📌 参数高效微调技术分类

- **增加式方法 (Additive Methods)**  
  向原模型中加入新的参数或模块进行微调，通常这些新增的参数或模块较小。比如，添加一些小的层或参数化层，以便在训练时只更新这些新增加的部分。
  例如适配器层（Adapter Layers）、提示微调（Prompt Tuning）、扩展模块（Expansion Modules）
  - Adapter Layers (适配器层)
    在原模型的各个层之间插入的微小神经网络模块
    典型方法：  
    - Low-Rank Adaptation (LoRA)：通过在模型的每个层之间插入低秩矩阵，来高效地微调模型。
    - BitFit：只调整模型偏置项，而不改变其他权重，减少训练的参数量。
  - Prompt Tuning (提示微调）  
    提示微调通过对模型输入添加学习到的“提示”或“预设”词嵌入，来引导模型的行为。这些额外的提示词是可以训练的参数，但模型的大部分权重不需要更新。  
    典型方法：  
    - Prefix Tuning：为每个输入序列添加一组可学习的前缀词嵌入，通过训练这些嵌入来调整模型的输出。
    - Prompt Tuning：通过学习一个固定长度的“提示词”向量，将其附加到输入数据中，从而引导预训练模型的行为。
 - Expansion Modules (扩展模块) 
    通过在模型的特定位置（如隐藏层）加入额外的扩展模块（例如卷积层、全连接层等）来增加模型的能力，而不是微调所有参数。这些扩展模块在训练过程中更新，但原始模型保持不变。

- **选择式方法 (Selective Methods)**  
  选择性地微调模型中的部分参数或层。通常会挑选出对任务影响最大的部分进行训练，以提高效率并减少计算成本。例如冻结层（Frozen Layers）、逐层微调（Layer-wise Fine-Tuning）
  - Frozen Layers (冻结层)
    冻结模型中的某些层或模块，只对部分层进行微调。通常将低层（如特征提取层）冻结，仅微调高层或任务相关层，从而减少计算量并防止过拟合。  
    典型方法：  
    -  冻结底层，微调顶部层：冻结模型的底层特征提取层，微调任务特定的顶部分类层或生成层。
    -  Gradual Unfreezing (逐步解冻)：在训练过程中逐步解冻更多层，通常从顶部开始，逐渐深入。
  
  - Filter-based Methods (基于滤波的方法  
    选择性地微调网络中的某些权重或滤波器。可以通过技术（如L1正则化）筛选出对任务最相关的特征或参数，减少需要训练的参数数量。
  
  - Layer-wise Fine-Tuning (逐层微调) 
    逐层选择性地微调网络的层次结构，通常是从网络的顶层开始，逐步向低层扩展。这样做可以逐步优化网络而不必一次性调整所有层的参数。


- **重新参数化方法 (Reparameterization Methods)**  
  通过变换模型的原始参数，采用低秩分解或参数共享等技术，使得模型在微调过程中只需要优化少量的参数，从而提高效率。如低秩分解（Low-Rank Decomposition）、权重共享（Weight Sharing）、量化（Quantization）、稀疏微调（Sparse Fine-Tuning）
  - Low-Rank Decomposition (低秩分解)
    通过将大矩阵（如权重矩阵）分解为两个小矩阵的乘积，减少需要训练的参数数量。低秩分解不仅减少了参数量，还可以提高训练的速度和内存使用效率。  
    典型方法：  
    - LoRA (Low-Rank Adaptation)：通过在模型的每个层插入低秩矩阵来减少微调的参数数量，LoRA的关键思想是仅训练插入的低秩矩阵。
  
  - Weight Sharing (权重共享)
    将模型的多个参数或层之间的权重进行共享，从而减少需要优化的参数数量。例如，将多个层的权重共享，或在模型的多个部分使用相同的权重矩阵。
  
  - Quantization (量化)
    将模型的参数从高精度的浮动点数值转换为低精度表示（如整数、二进制等），以减少存储需求和计算开销。通过量化技术，能够在减少参数数量的同时保持模型的有效性。
  
  - Sparse Fine-Tuning (稀疏微调)
    通过将参数矩阵稀疏化，仅保留重要的参数进行训练，减少了计算和存储需求。通常采用稀疏化技术（如L1正则化）来确定哪些参数是重要的。

- **特定模块微调**：如嵌入层微调（Embedding-based Fine-Tuning）、分类器微调（Classifier-based Fine-Tuning）。
    这种方法关注模型中的特定模块进行微调，避免对整个模型进行全面更新，从而提高训练效率。
    - Embedding-based Fine-Tuning (基于嵌入的微调)
      仅微调模型中的嵌入层（如词嵌入、位置嵌入等），而不改变模型的其他层。嵌入层通常是模型中的参数较少的部分，因此这种方法能有效减少计算开销。
    
    - Classifier-based Fine-Tuning (基于分类器的微调) 
      仅对模型的输出层（例如分类器、回归器）进行微调，而保持其他部分不变。适用于预训练模型已经能够提取到足够好的特征，任务仅需进行简单的输出调整。

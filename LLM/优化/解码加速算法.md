![image](https://github.com/hinswhale/AI-Learning/assets/22999866/b56ddb7d-2ddb-47a8-9cfd-b147c98ecdc0)# 解码加速算法
## 指标

### GPU 算力和 带宽

`算力`： GPU 每秒能够进行的浮点运算次数，单位是 FLOP/s；

`带宽`：该显卡每秒能够进行的显存读写量，单位是 byte/s。

`计算强度上限` 𝐼𝑚𝑎𝑥：算力和带宽的比值，单位为 FLOP/byte。

`模型的理论性能` ：最关心的指标，即模型在计算平台上所能达到的每秒浮点运算次数（理论值）。单位是FLOPS or FLOP/s。

### 运算量和访存量

`运算量`是指运行该模型需要的总浮点计算数，单位为 FLOP

`访存量`是运行该模型的过程中所需的显存读写量，单位为 byte

运算量和访存量的比值被称为该模型的`计算强度 𝐼`，单位为 FLOP/byte

`带宽瓶颈`：  计算强度 𝐼 < GPU 的计算强度上限 𝐼𝑚𝑎𝑥 , 说明 GPU 的理论最高显存读写速度低于实际运
算所需速度, 受到`带宽`影响

`计算瓶颈`: 𝐼 大于 𝐼𝑚𝑎𝑥 ，说明 GPU 的理论最高浮点运算速度低于实际运算所需速度, 受到`算力`影响

## Roof-line 的形态：
算力决定“屋顶”的高度（绿色线段）
带宽决定“房檐”的斜率（红色线段）
![image](https://github.com/hinswhale/AI-Learning/assets/22999866/d9452ee2-af04-4373-9582-9ceaf6ba2cf1)

Roof-line 划分出的两个瓶颈区域
![image](https://github.com/hinswhale/AI-Learning/assets/22999866/7db542ab-a49a-4a9e-a18c-d48243228e46)

### 计算瓶颈区域 Compute-Bound
此时HBM访问所花费的时间相对较低，不管模型的计算强度 有多大，它的理论性能 最大只能等于计算平台的算力。
例如，具有较大内维数的矩阵乘法和具有大量通道的卷积。

### 带宽瓶颈区域 Memory-Bound
当模型的计算强度小于计算平台的计算强度上限 时，由于此时模型位于“房檐”区间，因此模型理论性能的大小完全由计算平台的带宽上限 （房檐的斜率）以及模型自身的计算强度所决定。
例如，elementwise 操作 (如activation, dropout 等) 和 规约操作 (如sum, softmax, batch normalization, layer normalization等)。

`roofline模型讲的是程序可以达到的最好性能，而不是实际达到的性能。例如矩阵乘法，计算密度是N。但是因为cache大小的限制，GEMM实现的优劣，硬件的其他限制，你未必（或者说，几乎并不能）能达到roofline模型所定义的边界（屋顶）。`
以 A100 (40GB HBM) 为例，下面显示其内存层次结构的粗略图。SRAM内存分布在108个流式多处理器(SMs)上，每个处理器192KB。，在计算方面，使用Tensor Core的BFLOAT16 的理论峰值吞吐量为 312 TFLOPS。GPU 的典型操作方式是使用大量的线程来执行一个操作，这个操作被称为内核。

## RAM

RAM主要分为两类：SRAM（Static RAM）、DRAM（Dynamic RAM）。
* SRAM 只要存入数据后，即使不刷新也不会丢失记忆；而 DRAM 的电容需要周期性地充电，否则无法确保记忆长存。
* DRAM 密度高、成本低、访问速度较慢、耗电量大。SRAM 则刚好相反。
因此 SRAM 首选用于带宽要求高，或者功耗要求低的情境。如：CPU Cache、GPU On-Chip Buffer。DRAM 则一般用于系统内存、显存。
SRAM: 片上SRAM比HBM快得多，但比HBM小得多
DRAM

![image](https://github.com/hinswhale/AI-Learning/assets/22999866/690d4582-a752-4eef-adf6-1301a5f40c46)

# FlashAttention
![image](https://github.com/hinswhale/AI-Learning/assets/22999866/7494251d-7072-4d57-a7f4-da6a228373a4)
## 系统级优化

### FlashAttention

针对原始注意力模块的优化方案

通过矩阵分块和算子融合等方法，将中间结果一直保留在缓存中，直到获得最终结果后再写回显存中，从而减少了显存读写量。

### PagedAttention

针对键值缓存拼接和注意力计算的优化操作

### 批次管理优化

## 解码策略优化
### 推测解码（Speculative Decoding）
### 非自回归解码（Non-autoregressive Decoding）
### 早退机制（Early Exiting）
### 级联解码（Cascade Inference）


## 资料
1/  [LLM（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能](https://zhuanlan.zhihu.com/p/638468472)

2/ [GPU 内存概念浅析](https://zhuanlan.zhihu.com/p/651179378)

3/ [ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)

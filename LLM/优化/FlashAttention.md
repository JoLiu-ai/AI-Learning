# æ¦‚è¿°
FlashAttention - `Fast and Memory-Efficient Exact Attention with I0-Awareness`

å¯¹**Memory-Bound**çš„ä¼˜åŒ–
- ç›®æ ‡ï¼šå‡å°‘IOï¼Œå³å°½å¯èƒ½è®¿é—®GPUå†…ç¼“å­˜ï¼ˆå³**SRAM**ï¼‰
  - `åˆ†å—è®¡ç®—`ï¼Œ`fusionèåˆ`, `å‡å°‘ä¸­é—´ç»“æœç¼“å­˜`
  -  åå‘ä¼ æ’­æ—¶ï¼Œé‡æ–°è®¡ç®—ä¸­é—´ç»“æœ
 
- ç»“æœï¼š
  - æ— ç²¾åº¦æŸå¤± 
  - 2-4x speedup, 10-20x memory reduction
  - æ˜¾å­˜O(N^2) -> O(N)

![image](https://github.com/hinswhale/AI-Learning/assets/22999866/fc4177b7-df8e-40d1-921c-a39a73421435)

# é¢„å¤‡çŸ¥è¯†
## å¤§æ¨¡å‹å‚æ•°
- GPU memory:
  - P: Parameters
  - G: Gradients
  - OS: Optimizer states
> ä¸‰è€…å æ¯”ï¼š1:1:6

ä¼˜åŒ–å™¨çš„æ„é€ ä¼šå°è£… parametersï¼š
- optimizer = optim.Adam(model.parameters(), lr=0.001)
- loss.backward() => parameters.grad
- optimizer.step() => optimizer states
  - momentumï¼šgradient çš„æŒ‡æ•°å¹³å‡ ã€Adamã€‘
  - varianceï¼šgradient square çš„æŒ‡æ•°å¹³å‡ã€Adamã€‘
 
## GPU Memory
- SRAM[GPUå†…]  > HBM[GPUå¤–] > DRAM
  ![image](https://github.com/user-attachments/assets/fb1f825e-939d-446f-aef6-33298b162c74)

>  **ä¼˜åŒ–ç›®æ ‡**ï¼š å°½å¯èƒ½è®¿é—®GPUå†…ç¼“å­˜ï¼Œå³**SRAM**

- SMï¼ˆStream multiproecssorsï¼Œæµå¤šå¤„ç†å™¨ 
  - L1 cache - SRAM
  - register file -  SRAM

- SRAMï¼šStatic RAMï¼ˆRandom Access Memoryï¼‰ 192KB perï¼ˆA100 108ä¸ªï¼Œ4090 128ä¸ªï¼‰
 -  108*192/1024 = 20MB
- HBMï¼šhigh bandwidth memoryï¼ˆ4090 24GBï¼ŒA100 80GBï¼‰

## compute-bound vs. memory-bound
- compute-bound ï¼š
  -  å¤šç»´åº¦çš„çŸ©é˜µç›¸ä¹˜æˆ–æ˜¯é«˜ channel æ•°çš„ convolution
- memory-boundï¼š
  -  element-wise ï¼ˆe.g.ï¼Œ activationï¼Œ dropoutï¼‰ & reduction ï¼ˆe.g.ï¼Œ sumï¼Œ softmaxï¼Œ batch normï¼Œ layer normï¼‰

ä»è¿™ä¸ªå›¾å¯ä»¥çœ‹å‡ºGPT-2æ˜¯memory-bound
![image](https://github.com/user-attachments/assets/a3cd9e40-a734-4fb8-8867-fae8e57a39e5)

# **å…·ä½“æµç¨‹**

## æ ‡å‡†attentionè®¡ç®—


![image](https://github.com/hinswhale/AI-Learning/assets/22999866/6b85c509-7b16-4454-8b86-fd83d8d2c0b6)

ç¬¦å·è¯´æ˜: Q â€” queries, K â€” keys, V â€” values, S â€” scores, P â€” probabilities, O â€” outputs.

**ç“¶é¢ˆ**ï¼š

- æ¯æ¬¡æ“ä½œéƒ½è¦åœ¨HBMå’ŒSRAMç›´æ¥ç§»åŠ¨æ•°æ® [ NxNçŸ©é˜µï¼ˆSï¼ŒPï¼‰ï¼Œ N >> D, O(N^2)]


![image](https://github.com/user-attachments/assets/7a7eba39-acb2-44bf-9cdd-9aefd45d6a55)


## **Softmax**

$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$ 

## **Safe Softmax**

**å¦‚æœÂ ğ‘¥ğ‘–Â è¿‡å¤§ï¼Œå¯èƒ½å‡ºç°æ•°æ®ä¸Šæº¢çš„æƒ…å†µ**

$\text{Softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_{j} e^{x_j - \max(x)}}$ 

## æ ¸å¿ƒæ€æƒ³

- **åˆ†å—** +   **kernel fusion** å‡å°‘åœ¨HBMå’ŒSRAMä¹‹é—´æ•°æ®ä¼ è¾“æ¬¡æ•°

### Challenges:

- (1) Compute **softmax normalization** without access to full input.
- (2) Backward without the large attention matrix from forward.

### **Softmax normalization in blocks**

![æˆªå±2024-07-24 20.50.06.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/170c5405-f13e-403f-97ef-09c6d2e93955/%E6%88%AA%E5%B1%8F2024-07-24_20.50.06.png)

![æˆªå±2024-07-24 20.50.19.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/cf1f3a5b-8505-4452-8168-8f40ca94618c/9fecbfef-302c-426c-bc98-d1ab1c6a2edb/%E6%88%AA%E5%B1%8F2024-07-24_20.50.19.png)

> å›¾ç‰‡æ¥æºï¼šhttps://www.bilibili.com/video/BV1UT421k7rA/

### two main ideas

- **Tiling**ï¼ˆåœ¨å‰å‘å’Œåå‘ä¼ é€’ä¸­ä½¿ç”¨ï¼‰
    - Restructure algorithm to load block by block from HBM to SRAM to compute attention.
- **Recomputation**ï¼ˆä»…åœ¨åå‘ä¼ é€’ä¸­ä½¿ç”¨ - å¦‚æœæ‚¨ç†Ÿæ‚‰activation/gradient checkpointingï¼Œè¿™å°†å¾ˆå®¹æ˜“ç†è§£ï¼‰ã€‚
    - Don't store attn. matrix from forward, recompute it in the backward.

### ç®—æ³•æµç¨‹

![image](https://github.com/user-attachments/assets/24ca3d36-d1b6-46ee-87c4-b9387e11905b)


### tiling

- åˆ†å—

![image](https://github.com/user-attachments/assets/431eaffd-d6a4-4a7b-99a5-ca3bf7c7e10d)


> å›¾ç‰‡æ¥æºï¼šhttps://zhuanlan.zhihu.com/p/669926191

- **Softmax normalization**
![image](https://github.com/user-attachments/assets/f3348e86-5ef2-4a52-b8a6-5ff0be195590)

![image](https://github.com/user-attachments/assets/372f2fcf-e6d7-436f-8c42-d2fdcd76fd1c)


### Recomputation(backward pass)ã€todoã€‘

![./images/Inference_regular_attn.gif](./images/Inference_regular_attn.gif)

![./images/inference_splitkv.gif](./images/inference_splitkv.gif)

![./images/Inference_regular_attn.gif](./images/Inference_regular_attn.gif)

### flash & flash2


![image](https://github.com/user-attachments/assets/fdc50cbb-db70-407a-9740-9569195932cc)


# å‚è€ƒ

1. [Hardware-aware Algorithms for Sequence Modeling - Tri Dao](https://www.youtube.com/watch?v=foG0ebzuw34)
2. [flash-atttention-2](https://princeton-nlp.github.io/flash-atttention-2/)
3. [ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
4. [Flash Attention ä¸ºä»€ä¹ˆé‚£ä¹ˆå¿«ï¼Ÿ](https://www.bilibili.com/video/BV1UT421k7rA/)
5. [å›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼šFlashAttention V1ï¼Œä»ç¡¬ä»¶åˆ°è®¡ç®—é€»è¾‘](https://zhuanlan.zhihu.com/p/669926191)
